#### 쿠키와 세션의 차이에 대해 설명해 주세요.

![](https://velog.velcdn.com/images/sujipark2009/post/96097c10-bb46-40e5-b58f-8c29ce903189/image.png)

**Cookie**

클라이언트 로컬에 저장되는 키와 값이 들어있는 파일이다.

클라이언트의 상태정보를 브라우저에 저장하여 참조하는 기능을 하며

이름,값,유효 시간,경로 등을 포함하고

만료시간,쿠키 전송 도메인 이름,쿠키 전송 경로,보안 연결 여부(Secure),HttpOnly 여부 등을 담고있다.

웹 브라우저가 서버에 요청을 하면, 상태를 유지하고 싶은 값을 cookie로 생성하고 서버가 응답 시 HTTP헤더(Set-Cookie)에 쿠키를 포함해서 전송한다.

전달받은 쿠키는 웹 브라우저에서 관리하다가, 다음 요청 때 쿠키를 HTTP 헤더에 넣어서 전송하게되고

서버에서는 쿠키 정보를 읽어 이전 상태 정보를 확인 후 응답하게 된다.

예시로는, 사이트에서 로그인 시 "아이디와 비밀번호를 저장하시겠습니까?" 와
팝업창을 통해 "오늘 이 창을 다시 보지 않기" 를 체크하는 경우가 있다.

**Cookie 인증**

쿠키는 **key-value** 형식의 문자열 덩어리다.

클라이언트가 어떠한 웹사이트를 방문할 경우, 그 사이트가 사용하고 있는 서버를 통해 클라이언트의 브라우저에 설치되는 작은 기록 정보 파일이다. 각 사용자마다의 브라우저에 정보를 저장하니 고유 정보 식별이 가능한 것이다.

![](https://velog.velcdn.com/images/sujipark2009/post/134ff4c1-456a-4c7a-86e4-142544f1ef0f/image.png)

![](https://velog.velcdn.com/images/sujipark2009/post/8ddc7add-60bc-42be-ab65-75c8da0e3d6f/image.png)

Cookie 인증 방식의 과정을 보면,

1. 브라우저(클라이언트)가 서버에 요청을 보낸다.

2. 서버는 클라이언트의 요청에 대한 응답을 작성할 때, 클라이언트 측에 저장하고 싶은 정보를 응답 헤더의 Set-Cookie에 담는다.

3. 이후 해당 클라이언트는 요청을 보낼 때마다, 매번 저장된 쿠키를 요청 헤더의 Cookie에 담아 보낸다.

서버는 쿠키에 담긴 정보를 바탕으로 해당 요청의 클라이언트가 누군지 식별하거나 정보를 바탕으로 추천 광고를 띄우거나 한다.

**Cookie 방식의 단점**

가장 큰 단점은 보안에 취약하다는 점이다.

요청 시 쿠키의 값을 그대로 보내기 때문에 유출 및 조작당할 위험이 존재한다.

쿠키에는 용량제한이 있어 많은 정보를 담을 수 없다.

웹 브라우저마다 쿠키에 대한 지원 형태가 다르기 때문에 브라우저간 공유가 불가능하다.

쿠키의 사이즈가 커질수록 네트워크에 부하가 심해진다.

**Session**

일정 시간 동안 같은 브라우저로부터 들어오는 요청을 하나의 상태로 보고 그 상태를 유지하는 기술

웹 브라우저를 통해 서버에 접속한 이후부터 브라우저를 종료할 때까지 유지

웹 브라우저가 서버에 요청을 하면, 서버가 웹브라우저에 유일한 ID를 부여한다.

서버가 응답 시 HTTP헤더에 Session ID를 포함하여 전송하고

웹브라우저는 브라우저 종료 시까지 요청마다 Session ID가 담긴 쿠키를 헤더에 넣어 전송한다.

서버는 SessionID를 확인하여 해당 세션관련 정보를 확인한 후 응답한다.

**Session 인증**

위의 쿠키의 보안적인 이슈 때문에, 세션은 비밀번호 등 클라이언트의 민감한 인증 정보를 브라우저가 아닌 서버 측에 저장하고 관리한다. 서버의 메모리에 저장하기도 하고, 서버의 로컬 파일이나 데이터베이스에 저장하기도 한다.

핵심은, 민감한 정보는 클라이언트에 보내지 말고 서버에서 모두 관리한다는 점이다.

> 세션 객체는 Key에 해당하는 Session ID와 이에 대응하는 Value로 구성되어 있다.
> Value에는 세션 생성 시간, 마지막 접근시간 및 User가 저장한 속성 등이 Map형태로 저장된다.

![](https://velog.velcdn.com/images/sujipark2009/post/88cd591f-3a0f-4c34-9083-f9146c2b1acf/image.png)

![](https://velog.velcdn.com/images/sujipark2009/post/1edb7584-15b8-4fa6-9584-f314dd0e9e0d/image.png)

1. 유저가 웹사이트에서 로그인하면 세션이 서버 메모리(혹은 데이터베이스) 상에 저장된다.
   이때, 세션을 식별하기 위한 Session ID를 기준으로 정보를 저장한다.

2. 서버에서 브라우저의 쿠키에다가 Session ID를 저장한다.

3. 쿠키에 정보가 담겨있기 때문에 브라우저는 해당 사이트에 대한 모든 Request에 Session ID를 쿠키에 담아 전송한다.

4. 서버는 클라이언트가 보낸 Session ID와 서버 메모리로 관리하고 있는 Session ID를 비교하여 인증을 수행한다.

**Session 방식의 단점**

쿠키를 포함한 요청이 외부에 노출되더라도 세션 ID 자체는 유의미한 개인정보를 담고 있지 않는다.

그러나 해커가 **세션 ID 자체를 탈취**하여 클라이언트인척 위장할 수 있다는 한계가 존재한다.

서버에서 세션 저장소를 사용하므로 요청이 많아지면 서버에 부하가 심해진다.

**Cookie vs Session**

저장위치 : 쿠키는 클라이언트에 저장 / 세션은 서버에 저장

보안 : 쿠키는 클라이언트에 저장하므로 보안에 취약 / 세션은 쿠키를 이용해 Session ID만 저장하고 서버에서 처리하므로 비교적 안정

라이프사이클 : 쿠키는 만료시간에 따라 브라우저를 종료해도 계속해서 유지 가능 / 세션은 만료시간을 정할 수 있지만 브라우저가 종료되면 만료시간에 관계 없이 삭제

속도 : 쿠키는 클라이언트에 저장되어 서버 요청시에 빠른 속도 / 세션은 실제 저장된 정보가 서버에 있어 쿠키보다 느림

쿠키와 세션은 비슷한 역할을 하며, 동작 원리도 비슷하다. 그 이유는 세션도 결국 쿠키를 사용하기 때문이다.

#### 쿠키와 세션의 필요성은 무엇인가요?

---

HTTP 프로토콜의 특징으로 인해 모든 요청 간의 의존관계 확인 불가능
=> 같은 사용자의 요청인지 확인할 방법이 없음

연결을 유지하지 않아 리소스 낭비를 줄일 수 있지만, 매번 새로 연결하기 위해 클라이언트는 매번 인증을 해야 함

HTTP의 특징은 stateless를 해결하기 위해 쿠키와 세션을 사용

#### Stateless의 의미를 살펴보면, 세션은 적절하지 않은 인증 방법 아닌가요?

---

HTTP의 Stateless는 서버 통신의 비용을 줄이고, 확장성을 높이기 위한 특성이다.

초기의 HTTP는 단순히 HTML을 주고받기 위한 수단이었기에 지금과는 많이 달랐고..
현 시대의 웹에서는 불가피하게 요청과 요청간의 관계가 유지되어야 하는 상황이 많이 생기게 되었다.

비용에 관련한 Trade-off의 측면에서, 세션을 사용하면 오히려 클라이언트는 매번 인증 과정을 거쳐야 하는 문제를 줄일 수 있다.

Scale-out 상황의 확장성도 Session Clustering을 통해 해결이 가능하다!!

#### 쿠키에 아이디와 비밀번호를 넣어도 되나요?

---

결론부터 말하면 안된다.

클라이언트에 저장되기 때문에 Request를 보내는 과정에서 해커에 의해 탈취될 가능성이 있다.

이걸 극복하기 위해 세션인 토큰방식을 도입하게되는것이고.. 특히 토큰의 경우 암호화 알고리즘을 모르면 정보를 얻을 수 없다.

또한 HTTPS를 사용하는 방법이 있다..

#### 세션 방식의 로그인 과정에 대해 설명해 주세요.

---

![](https://velog.velcdn.com/images/sujipark2009/post/73af45b0-ffa8-4efa-a56a-6ed8b07cb84f/image.png)

1. 클라이언트가 로그인 요청을 보낸다.

2. 서버에서 DB에 저장된 ID/PW와 비교하여 올바르면 응답을 보낸다.

3. 응답을 보낼 때 Session 저장소에 세션정보를 생성하고 ID를 발급받는다.

4. 응답에 Session ID를 같이 담아서 클라이언트에게 보낸다.

5. 클라이언트는 Session ID를 쿠키에 저장한다.

6. 인증이 필요한 요청에 대해 요청마다 헤더에 쿠키를 넣어서 보낸다.

7. 서버는 쿠키에 있는 Session ID를 세션 저장소에서 찾아서 발견한 경우 해당 요청에 대한 응답을 보낸다.

#### 규모가 커져 서버가 여러 개가 된다면, 세션을 어떻게 관리할 수 있을까요?

---

**서버 다중화 환경에서의 세션 불일치**

단일 서버 환경에서는 session을 통한 로그인을 구현할 때 session 불일치 문제를
신경쓸 필요가 없다.

하지만 서비스가 커짐에 따라 한대의 서버로 운영하는것이 불가능해졌다고 한다면, 서버를 업그레이드 해야하는데 두가지 방식이 존재한다.

첫번째는 `scale-up` 방식이다.

**서버 자체 성능을 늘려** 부하를 견딜 수 있게 하는 방식이지만, 여전히 서버 한 대에 모든 트래픽이 집중되므로 만일에 서버 장애가 생길시 서버가 복구될 때까지 **서비스를 중단**해야 하는 상황이 발생할 수 있는 위험이 있다.

두번째 방법은 `scale-out` 방식이다.

**서버를 여러대로 늘려서** 각 서버에 로드밸런싱으로 트래픽을 분산하게 한다.

그래서 서버 한대에 장애가 생겨도 다른 서버는 살아있으니 서비스 문제가 생기지 않는다.

그러나 이때 서비스 이용에 발생하는 커다란 문제점이 있는데 바로 데이터 정합성, **세션 불일치 문제**이다.

왜냐하면 여러 대의 서버가 각각 세션 저장소를 독립적으로 갖기 때문에 데이터 불일치 문제가 발생하기 때문이다.

다음 예시를 보면,

![](https://velog.velcdn.com/images/sujipark2009/post/9b1db5ac-e71c-420d-bf3a-4ce851809308/image.png)

서버를 3대로 증강시키고 로드밸런서로 각 서버로 트래픽이 분산이 되게 운영중이다.

![](https://velog.velcdn.com/images/sujipark2009/post/f7d2681e-95d3-49aa-b8a0-d6dd2fbd9bb5/image.png)

이때 사용자가 서버1에 접속해 로그인을 진행하여 서버1에 세션이 저장된다.

![](https://velog.velcdn.com/images/sujipark2009/post/e903d808-91bb-4812-9281-bd497b1f009c/image.png)

로그인 한 후 게시판에 글을 쓰기 위해 서버에 요청했는데, 로드밸런싱 알고리즘으로 인해 서버1이 아닌 서버2로 요쳥되어진다.

![](https://velog.velcdn.com/images/sujipark2009/post/f778f364-8d82-40bb-82f7-5efd99b0db8d/image.png)

하지만 서버2에는 사용자의 로그인 세션이 없기 때문에 또 로그인하라는 응답을 보내게 된다.

이처럼, 서비스 안정을 위해 서버를 늘렸더니 서비스 품질 자체가 떨어지는 결과를 맞이해버렸다..!

이러한 세션의 **데이터 정합성** 문제를 해결하기 위한 방법에는 무엇이 있는지 살펴보자.

**Stick Session 방식**

간단하게 생각해보면 로드밸런서가 랜덤으로 아무 서버에게 요청을 보내서 발생된 문제이니까, 만일 클라이언트의 요청이 어느 한 서버에 도달해 세션 데이터가 생겼다면, 앞으로 **이 서버는 해당 클라이언트만의 요청/응답만 처리하도록 고정**해주면 된다.

![](https://velog.velcdn.com/images/sujipark2009/post/7bad36db-0d0f-407e-a5dc-efd18a50f7d6/image.png)

Stick Session이란 말 그대로 고정된 세션을 의미한다.

![](https://velog.velcdn.com/images/sujipark2009/post/64cf9801-ad71-44ca-bb17-58a74be94d6b/image.png)

1. 클라이언트는 서버에게 처음 요청을 전달한다.
   그러면 로드밸런서는 서버들 중 하나에게 요청을 보내 처리한다.

2. 서버에서 클라이언트에게 응답을 보낼 때, Set-Cookie : ServerId=서버1 이런 형태로 정보를 담아 보낸다.

3. 이후 클라이언트가 다시 서버에 요청을 보낼 때 Cookie : ServerId=서버1 을 함께 보낸다.

그러면 로드 밸런서가 우선적으로 요청에 쿠키가 있는지부터 확인하고, 쿠키의 정보를 확인했다면 해당 요청은 해당 쿠키가 생성되어있는 서버로 보내지게 된다.

4. 만약 존재하지 않는 쿠키라면, 로드 밸런서의 알고리즘에 의해 선정된 다른 서버에 쿠키가 생성되어 다음에 똑같은 요청이 오면 같은 경로로 매핑시켜 줄 수 있도록 한다.

이렇게 동일한 사용자가 세션이 있는 해당 서버에 계속 요청을 보낼 수 있도록, 지속적으로 서버 정보가 쿠키를 통해 응답에 삽입되어 보내지게 되어 클라이언트와 서버가 서로 연결을 유지할 수 있는 것이다.

이러한 방식을 사용하면 유저는 세션에 대한 데이터 불일치 문제에서 자유로워질 수 있게 된다.

> AWS ELB(Elastic Load Balancer)에서 대표적으로 제공한다.

**Sticky Session 문제점**

**특정 서버에 트래픽이 집중되는 문제**

고정된 세션을 사용한다는 말은 사용자가 접속해야 하는 서버가 고정되어 있기 때문에 하나의 서버에 트래픽이 집중될 수 있다는 위험성을 갖고 있다는 말이다.

즉, 본래 목적인 로드 밸런싱이 제대로 이뤄지지 않을 수 있다.

**세션 정보의 유실**

서비스 중에 만일 하나의 서버에 장애가 발생하게 되면 해당 서버를 사용하는 사용자들은 세션정보를 모두 잃게 된다.

이렇게 되면 다른 서버에서 세션 인증을 다시 해야하는 문제가 생기게 된다.

이처럼 Sticky Session에서는 사용자와 세션 정보를 갖고있는 서버를 1:1로 매핑해주어 세션 불일치를 해결하지만, 문제가 발생하면 Scale-out의 장점인 트래픽 분산과 가용성을 제대로 활용하지 못하게 되는 경우가 발생할 수 있게 된다.

**Session Clustering 방식**

앞서 Sticky session은 각 서버에 세션을 저장해놓았더니 세션 불일치는 해결되었으나, 도리어 성능이 안좋아진다는 결과를 낳았다.

그러면 세션 정보를 각 서버마다 저장하는게 아닌, **세션 데이터를 복사하여 서버들에게 전파**해 가져다 쓸 수 있으면 되지않을까?

![](https://velog.velcdn.com/images/sujipark2009/post/a63e7612-3f0c-42cd-90c9-3b538c71d0c9/image.png)

세션 클러스터링은 서버들을 하나의 클러스터로 묶어 관리하고, **클러스터 내의 서버들이 세션을 공유**할 수 있도록 하는 방식이다.

예를들어 서버1에서 session이 저장되었다면, 서버2와 서버3에도 서버1에 저장되어있는 세션을 전파(복사)하는 것이다.

WAS마다 Session Clustering을 지원하는 방식이 조금씩 다르다고 하는데..

우리가 사용하는 WAS인 톰캣에서는 어떻게 구현하는지 알아보자.

**Tomcat Session Clustering**

Tomcat에서는 크게 2가지 방식으로 Session Clustering 구현기능을 제공한다.

1. All-to-All Session Replication

**All-to-All** 방식은 하나의 세션 클러스터 내에서 데이터가 변경되면 변경된 사항이 다른 모든 서버로 복제되는 방식으로, 톰캣에서 제공하는 DeltaManager 클래스를 통해 구현된다.

특정 서버에 생성된 세션을 **클러스터를 이루는 모든 서버에 세션을 복제**하기 때문에 클라이언트의 요청을 한 곳으로 지정하지 않아도 되고 다른 서버로 요청을 보내더라도 같은 세션을 유지할 수 있다.

만일 이용하고 있는 서버에 장애가 발생해도 다른 서버에서 세션을 유지하고 있기 때문에 클라이언트는 동일한 서비스 환경을 제공받을 수 있게 된다.

하지만, **All-to-All** 방식에서는 모든 서버가 전체 세션 데이터를 유지하고 있기 때문에, 다른 서버에서 세션을 찾기위한 추가적인 네트워크 I/O가 발생하지는 않지만, 그만큼 **많은 메모리가 필요** 하다는 단점이 있다.

그리고 세션을 저장할 때 서버 수 만큼 복제하고 각 서버에 전달, 저장해야하기 때문에 서버 수에 비례하여 **네트워크 트래픽이 증가**하게 되기도 한다.

추가적으로 세션 전파 작업 중 모든 서버에 **세션에 전파되기까지의 시간차로 인한 세션 불일치 문제**와 같은 예상치 못한 문제가 발생할 가능성이 존재한다.

> Tomcat 공식 문서에서도, All-to-All 방식은 소규모 클러스터 환경(노드가 4개 미만)에서 좋고, 이보다 큰 클러스터 환경에서는 추천하지 않는다고 한다.
> 그리고 DeltaManager를 사용한 방식은 애플리케이션이 배포되지 않은 노드에도 복제를 시도하기 때문에 불필요한 트래픽을 서버에 발생시키는 문제도 존재한다고 함.
> Tomcat에서는 이러한 문제를 해결하기 위해 BackupManager를 이용한 방법도 제공해준다.

2. Primary-Secondary Session Replication

![](https://velog.velcdn.com/images/sujipark2009/post/c9043509-486c-4bd7-ae25-1c9a28485c4f/image.png)

**Primary-Secondary** 방식은 Primary서버의 세션데이터를 Secondary(Backup)서버에만 전체 복제하여 저장하는 방식으로, BackupManager 클래스를 통해 이 방식을 제공하고 있다.

All-to-All Session Replication 방식은 그냥 무식하게 모든 서버에 복제하고 저장하는 것과 달리, Primary 서버와 Secondary(Backup)서버에만 전체 세션을 복제하여 저장하되, 나머지 이외의 서버들에는 세션의 Key에 해당하는 JSESSIONID만 복제, 저장함으로써 메모리를 절약할 수 있는 방식이다.

하지만 만일 Primary,Secondary 서버를 제외한 다른 서버에 세션 정보를 요청할 경우 다시 온전한 세션 정보를 얻기 위해서는 Primary,Secondary에 다시 요청을 보내야한다는 문제점이 존재한다.

> Primary Secondary Session Replication 방식은 비교적 대규모 클러스터 환경에서 적합한 방식이라고 한다.

**Session Clustering 문제점**

위에서 톰캣 세션 클러스터링을 설명하면서 간간히 문제점에 대해 언급했지만 정리하면,

**서버 세팅의 어려움**

이 방식은 scale out 관점에서 서버가 하나 뜰 때마다 기존에 존재하던 WAS에 새로운 서버의 IP/Port를 입력해서 클러스터링 해줘야 하는 문제점이 있다.

**추가 메모리 비용**

서버마다 동일한 세션 정보를 가지고 있어야 하기 때문에, 서버가 확장될 수록 복제해야 할 세션 데이터가 늘어나고 이는 추가적인 오버헤드로 이어진다.

Tomcat을 예로 들었을 때, 모든 데이터를 각각의 Tomcat 노드에게 전달해야 하고 배포하는 노드가 아닐 경우에도 복사를 진행하기 때문에 불필요하게 메모리를 차지한다.

**네트워크 트래픽 증가**

데이터 변경이 발생할때 마다 세션을 전파(복사)하는 작업이 일어나기 때문에 네트워크 요청 트래픽이 증가하게 된다.

**시차로 인한 세션 불일치 발생**

세션 전파 작업 중 모든 서버에 세션이 전파되기까지의 시간차로 인한 세션 불일치와 같은 예상치 못한 문제가 발생할 가능성이 존재한다.

이처럼 세션 클러스터링은 Sticky session의 문제점인 특정 서버에만 트래픽이 몰리는 문제를 해결할 수 있었다.

그러나 세션 클러스터링이나 Sticky Session이나 **서버가 세션이라는 상태(데이터)를 가진다는 것**은 변함이 없다는 특징이 있다.

`서버가 상태를 가진다` 라는 의미는 scale-out 방식으로 확장을 했을 때 서버가 가지고 있는 데이터를 확장하는 서버에도 똑같이 맞춰줘야 한다는 뜻이다. 이는 곳 overhead로 이어진다.

정리하면, 세션 클러스터링은 정합성 이슈를 해결할 수 있지만 성능적인 한계가 존재한다고 할 수 있다.

**Session Storage 방식**

그렇다면 위의 두 방식의 단점을 보완하여 다중 서버에서 세션을 공유할 수 있는 방법이 없을까?

모든 서버에 일일이 세션 메모리를 복제/저장하는것이 낭비라면, 그럼 별도의 세션 저장소를 외부에서 생성하고 각 서버들이 가져와 사용하면 되지 않을까?

![](https://velog.velcdn.com/images/sujipark2009/post/b811225e-ed1a-437c-ac0d-ddea1c52a611/image.png)

세션 스토리지는 기존의 서버 내 세션 저장소를 이용하지 않고, 로컬 서버에서 분리해 **별도의 세션 저장소를 두고 서버들이 이를 공유**함으로써 세션 불일치를 해결하는 방식이다.

따라서 새로운 서버를 추가하더라도 추가한 서버에만 세션 저장소 정보를 명시해주기만 하면 되기 때문에 **기존 서버의 수정이 발생하지 않는다는 장점**이 있게 된다.

그래서 세션을 저장할 때 세션을 복제해 다른 서버들에 보낼 필요가 없어 **WAS들끼리 불필요한 네트워크 I/O 과정을 진행하지 않아**도 되어 성능면에서 유리하다.

또한, 한 서버에 장애가 발생하더라도 세션은 이와 독립되어 별도로 존재하기 때문에 세션을 활용한 서비스에 영향을 미치지 않는다.

**Session Storage 종류**

세션 Storage종류에는 2가지가 있다.

첫번째로 Disk 방식으로 되어있는 DB가 있고 두번째로 In-Memory 기반의 데이터베이스가 있다.

1. Disk Database(MySQL,Oracle)

말 그대로 세션 데이터를 디스크에 저장하는 것이다.
따라서 전원이 공급이 안되도 디스크에는 정보를 잃지않고 잘 유지한다.

하지만 큰 단점이 하나 있는데, 속도가 너무 느리다는 것이 맹점이다.

2. In-Memory(Redis / Memcached)

In-Memory는 데이터를 메모리에 저장하는 방식이다.

그래서 I/O속도가 디스크와 비교해서 매우 빠르다.

하지만 이것또한 단점이 있는데, 전원이 공급되지 않으면 기억하고 있는 데이터를 모두 잃어버리게 된다.

두 가지 방식중에 세션 데이터를 저장하는 관점에서 어떤것이 좋을까?..

생각해보면 Session은 영구적인 저장이 필요하지 않는다.
예를들어 개인정보가 더 중요한 요즘은 지정한 시간을 넘기면 자동으로 로그아웃되게 처리되어 있다.

그렇기 때문에 세션 정보는 영구적인 정보 저장을 약속한다는 디스크 방식의 DB에는 매력적이지 않을 것 같다.

대신에 디스크에 비해 I/O속도가 빠른 In-Memory방식이 더 적합해보인다.

**Session Storage 문제점**

**문제가 생기면 모든 서버가 장애**

세션 클러스터 같은 경우 하나의 서버에 장애가 터져도 나머지 서버에 미리 복제를 해두었기 때문에 문제는 없다.

그러나 세션을 저장하고 있는 Session Storage 자체에 장애가 발생할 경우 모든 세션을 잃어버려 세션을 사용하는 모든 서버에 영향을 끼치는 위험이 있다.
그래서 이러한 문제를 보완하기 위해 동일한 세션 저장소를 하나 더 구성하는 방법(Master-Slave)으로 해당 문제를 해결하곤 한다.

**성능적인 마이너스**

별도의 Session storage로부터 세션을 불러와야 하기 때문에 추가적인 네트워크 I/O가 발생한다는 점이다.

세션을 외부에서 가져와 사용하기 때문에 로컬 메모리에 저장해 사용하는 것보다 성능적인 면에서 떨어질 수 밖에 없다.

#### 웹 브라우저가 서버로 URL을 요청했을 때, 흐름을 설명해주세요

---

1. 웹 브라우저가 URL을 파싱

- 브라우저에서 어떤 프로토콜,URL,포트로 요청할 것인지 해석 및 분석

2. URL이 문법에 맞으면 Punycode encoding을 url의 host부분에 적용

퓨니코드(Punycode)란, 한글과 한자는 유니코드를 사용하는데 이를 ASCII로 변환하는 것을 말한다.

(예시, www.긴급재난지원금.kr -> www.xn--jj0bb2kr6h965bxcbp8g.kr)

참고로 도메인명은 ASCII를 사용한다.

> 퓨니코드(Punycode)란?
> 기본적인 도메인들은 국제화 도메인에 맞는 ASCII 문자들을 이용해요.
> 한글과 한자는 유니코드를 이용하기 때문에 이것을 규격에 맞게 ASCII 코드로 변경해 주어야 하는데, 이 때 변경된 코드를 퓨니코드라고 해요.

3. HSTS(HTTP Strict Transport Secuiry)목록 조회

- HSTS는 HTTP를 허용하지 않고 HTTPS를 사용하는 연결만 허용하는 기능
- 만약 HTTP로 요청이 왔다면 HTTP 응답 헤더에 `Strict Transport Security`라는 필드를 포함하여 응답

- 이를 확인한 브라우저는 해당 서버에 요청할 때 HTTPS만을 통해 통신
- 그리고 자신의 HSTS 캐시에 해당 URL을 저장하는데, 이를 HSTS 목록이라고 한다.
- 이를 통해 브라우저에서는 이 HSTS 목록 조회를 통해 해당 요청을 HTTPS로 보낼지 판단

- HSTS목록에 해당 URL이 존재한다면 명시적으로 HTTP를 통해 요청한다고 해도 브라우저가 이를 HTTPS로 요청한다.

4. URL을 IP주소로 변환

- www.naver.com 이라는 주소로는 컴퓨터끼리 통신하지 않는다.
- 우선 브라우저에거 자신의 로컬 hosts파일과 브라우저 캐시에 해당 URL의 존재여부 확인

- 존재하지 않는다면 DNS서버에 요청하여 해당 URL을 IP로 변환한다.

5. 라우터를 통해 해당 서버의 게이트웨이까지 이동

- DNS 서버에게 IP주소를 받았으니 서버로 요청한다.
- 10.20.30.6이라는 IP라고 가정하면

- 해당 IP주소로 가야하는 것은 알지만, 어떻게 가야 할지 경로는 알 수 없다.
- 이 요청이 네트워크를 타고 어떻게 이동할지는 네트워크 장비인 `라우터`의 `라우팅`을 통해 이루어진다.

![](https://velog.velcdn.com/images/sujipark2009/post/fc6d7d62-38cf-4224-b721-d06ba5b71186/image.png)

6. ARP를 통해 IP주소를 MAC주소로 변환

- 실질적 통신을 하기 위해서는 논리주소인 IP를 물리주소인 MAC으로 변환해야한다.
- 이를 위해 해당 네트워크 내에서 ARP를 브로드캐스팅 한다.

- 해당 IP주소를 가지고 있는 노드는 자신의 MAC주소를 응답한다.

7. 대상 서버와 TCP 소켓 연결

- 대상서버와 통신을 하기 위해 TCP 소켓 연결을 진행한다(3 way handshake)

- 클라이언트에서 서버에 연결요청(SYN), 이 때 클라이언트는 Closed, 서버는 LISTEN 상태

- 서버가 SYN을 받으면 ACK데이터와 함께 클라이언트쪽에서도 포트를 열어달라는 SYN을 전송
  (서버는 처음에 LISTEN 상태에서 요청을 받고, SYN+ACK이후엔 상태가 SYN_RCV 상태가 된다)

- 클라이언트에서 서버로부터 SYN+ACK을 받으면 포트를 열고 이에 대한 응답으로 ACK를 전송한다(클라이언트는 ESTABLISHED 상태가 된다)

- ACK 데이터를 받은 서버 역시 ESTABLISHED 상태가 되면서 클라이언트와 서버는 연결이 된다.

![](https://velog.velcdn.com/images/sujipark2009/post/f7a00fdc-500a-48c2-b1d7-18be4048053d/image.png)

HTTPS 요청인 경우, TLS 핸드쉐이킹이 추가된다

8. HTTP(HTTPS) 프로토콜로 요청,응답

9. 웹 브라우저에서 응답을 해석

#### URL,URI,URN은 어떤 차이가 있나요?

---

![](https://velog.velcdn.com/images/sujipark2009/post/864b6c41-8867-4da0-bc08-0a802e41ce29/image.png)

**URI**

Uniform Resource Identifier(통합자원식별자)는 인터넷에 있는 자원을 어디에 있는지 자원 자체를 식별하는 방법이다.

인터넷의 자원 자체를 식별할 수 있는 문자열

예시 : example.co.kr

**URL**

Uniform Resource Locator(파일식별자)는 네트워크 상에서 자원이 어디있는지 위치를 알려주기 위한 규약이다.

네트워크상에서 리소스의 위치를 나타내기 위한 규악.
식별자와 위치를 동시에 나타냄

URI의 일종이다.

특징으로는, 프로토콜과 결합한 개념이다(HTTP,FTP,SMTP 등)

예시 : https://example.co.kr

웹 상의 주소를 효율적으로 나타내는 방식이기에, 더 효율적으로 리소스에 접근하기 위해 클린한 URL 작성을 위한 방법이 생겨났고 그 중 하나가 `REST API`이다

![](https://velog.velcdn.com/images/sujipark2009/post/2c4fe1cb-a5c8-43bc-a8d6-bc650caf9ab5/image.png)

**URN**

Uniform Resource Name(통합자원이름)은 url:scheme을 사용하는 URI를 위한 역사적인 이름이다

URL이 리소스가 있는 위치를 지정한다면, URN은 리소스에 이름을 부여하는 것이다.

리소스 위치,프로토콜,호스트와 관계 없이 이름을 부여한 것

리소스를 영구적이고 유일하게 식별할 수 있는 URI

실제 자원을 찾기 위해서 URN을 URI로 변환하여 ㅅ용

리소스를 어떻게 가져오는지 보다는 리소스를 특정하기위한 문자열

![](https://velog.velcdn.com/images/sujipark2009/post/210dad50-162b-45a0-8774-9e9358603744/image.png)

![](https://velog.velcdn.com/images/sujipark2009/post/7411e036-a8f8-4804-b859-37d9c29fc5e1/image.png)

그래도 모호한데..

자원의 식별자 URI와 자원의 위치 URL은 언뜻 보면 같은 것을 의미하는 것 같지만, 자원의 위치라는 것은 결국은 하나의 파일 위치를 나타내는 것임을 명심하자.

예를들어 다음과 같은 홈페이지 링크가 있다고 하면,

https://www.naver.com/index.html?page=1&id=1

https://www.naver.com/ 서버에 위치한 index.html 페이지는 query string인 page의 값에 따라 여러가지 화면 결과를 나타내게 된다.

이때 여기서 URL은 index.html의 위치를 표기한 https://www.naver.com/index.html 까지이다.

하지만 사용자가 원하는 정보에 도달하기 위해서는 ?page=1&id=1 이라는 식별자(identifier)가 필요한 것이다.

따라서 엄격히 구분하자면 위의 https://www.naver.com/index.html?page=1&id=1 주소는 URI이고, 식별자가 빠진 https://www.naver.com/index.html 을 URL이라고 해야한다.

이유는 URL은 **자원의 위치**를 나타내 주는 것이고 **URI는 자원의 식별자**인데, ?page=1&id=1 이 부분은 위치를 나타내는 것이 아니라 page값이 1이고 id가 1인것을 나타내는 식별하는 부분이기 때문이다.

물론 통상적으로 대충 URL이라고 하지만, 엄격하게 URI라고 하는게 맞다.

억지스러운 예를 들면, 아래의 두 주소는 같은 URL이지만 다른 URI라고 할 수 있다.

https://www.naver.com/index.html?page=1&id=1
https://www.naver.com/index.html?page=1&id=2

#### 웹 서버와 웹 어플리케이션 서버의 차이는 무엇인가요?

---

사용자의 요청을 처리하는 방식과 기능에 차이가 있다.

**웹 서버**

클라이언트(보통 웹 브라우저)로부터 HTTP요청을 받아 HTML문서나 간단한 리소스들을 서비스하는 역할

정적인 컨텐츠를 제공 : 서버에 미리 저장되어 변경되지 않는 데이터

예시 : HTML,CSS,JavaScript,이미지 파일 등을 클라이언트에게 전송해주는 역할

웹 서버의 종류 : Apache,Nginx 등

#### **Apache와 Nginx의 차이**

`Nginx`는 웹서버로, 가볍고 성능이 뛰어나다는 장점이 있다.
기존 1위 웹서버인 `apache` 에 비교해 다중 커넥션 제어에 유리하다.

다중 커넥션 제어에 유리하다는 말은, 많은 유저의 요청을 효율적으로 처리할 수 있다는 이야기이다.

이는 **이벤트 드리븐** 이라는 Nginx의 특징에서 기인한다.

**Apache**

웹 서버는 다양한 유저가 접속하기에 다중 유저 요청을 이상없이 처리해야 한다.
아파치는 unix 계열의 OS가 네트워크 커넥션을 생성하는 방식과 유사하게, 유저 요청이 들어오면 하나의 프로세스를 할당해 처리하게 구현하였다.

프로세스를 생성하는 비용은 부하가 상당하기 때문에, 프로세스를 미리 생성해서 할당하는 `PREFORK` 방식을 활용하였다.

![](https://velog.velcdn.com/images/sujipark2009/post/7f7b6c6c-e159-49b5-bb9b-2bdc90208965/image.png)

유저 하나가 접근하면 PREFORK 과정을 통해 생성해놓은 프로세스 하나를 할당해 유저의 HTTP 요청을 처리한다.

만약 PREFORK로 만들어놓은 프로세스가 이미 누군가의 요청을 처리하고 있다면 새로운 Process를 만들어서 처리하게 된다.

유저 요청 하나에 대해 프로세스 하나를 할당하는 이 방식을 `Process Driven` 이라고 부르기도 한다.

**C10K 문제**

PC의 보급률이 높아지며 많아진 트래픽으로 인해, 문제가 생겼다..

클라이언트로부터 커넥션을 처리할 프로세스를 새롭게 생성해주지 못하는 문제가 생긴 것.

이것을 두고 `커넥션 10,000개 문제` 줄여서 `C10K`문제라고 불렀다.

문제는 Apache의 내부 구조 때문이었는데..

Apache의 장점이었던 모듈을 쉽게 얹을 수 있었던 점으로 인해 서버 프로세스가 무거워지고, 무거운 프로세스를 새로운 유저 connection 수 만큼 생성하니 메모리가 버티지 못한 것이였다.

또 수많은 프로세스가 끊임없이 CPU를 컨텍스트 스위칭 하는 비용도 상당했다.

**Nginx의 등장**

Nginx는 아파치 앞단에 두어 아파치가 받을 커넥션을 줄여줄 목적으로 만들어졌다.

Nginx는 그 자체로 웹서버이기 때문에 HTML,CSS,JS,이미지 등의 정적파일은 직접 반환할 수 있다ㅏ. 그리고 동적으로 처리해줘야할 요청만 Apache로 보냄으로써 connection을 줄일 수 있었다.

![](https://velog.velcdn.com/images/sujipark2009/post/ef9c8aef-7d1b-4072-8449-5bd66243eae5/image.png)

nginx에는 keep-alive 설정으로 여전히 연결되어 있지만, apache로 보낼 때에는 별도 요청으로 받아옴으로써 해당 커넥션을 유지시키지 않을 수 있었다.

그렇다면,Nginx는 다중 커넥션이 연결되어 있는데도 apache처럼 프로세스 문제는 없었을까?

**마스터 프로세스,워커 프로세스**

비결은 `요청이 많이 들어와도 프로세스를 늘리지 않는다` 이다.

Nginx는 설정파일을 읽고 워커 프로세슬르 생성하는 역할을 하는 마스터 프로세스와 실제로 유저 요청을 처리하는 워커 프로세스로 이루어져 있다.

Nginx가 구동되면 마스터 프로세스는 정해진 숫자만큼의 워커 프로세스를 생성하고, 소켓을 배정하게 된다.(워커 프로세스와 통신할 수 있는 Socket descriptor)

그러면 워커 프로세스의 숫자가 정해져있는데 어떻게 다중 커넥션의 요청을 동시에 처리할 수 있을까?

**이벤트 드리븐(Event Driven)**

이벤트 드리븐에서는 TCP(or UDP)connection의 연결, 유저의 Request처리, Connection 종료까지의 모든 절차를 `이벤트`라는 개념으로 취급하고 처리한다.

![](https://velog.velcdn.com/images/sujipark2009/post/c0fda150-82b3-4973-a732-ab5c626bdaed/image.png)

그리고 워커 프로세스에게 working queue라는 이름의 처리해야 할 작업이 순차적으로 담긴 큐를 처리하도록 한다. 이렇게 구현하면 워커 프로세스가 놀고 있는 시간 없이 끊임없이 이벤트(커넥션 연결,종료,요청 처리...)를 처리하게 된다.

커넥션에서 아무런 요청도 들어오지 않고 대기하고있는 keep-alive 상황에서는 하는 일 없이 메모리만 축내고 있던 apache의 프로세스와는 대조적이다.

만약 오래걸리는 작업으로 인해 이벤트들이 Blocking이 된다면?
예를들어 disk I/O같은 시간이 오래 걸리는 작업이 들어온다면, 해당 작업이 처리되는 동안 working queue에 있는 다른 작업들을 할 수 없게 된다..

자 이부분을 알아보자..

기본적으로 read()나 write() 시스템 콜이 호출되면 작업이 완료될 때 까지 해당 스레드가 아무 작업도 수행할 수 없다.

왜냐하면 read(),write()같은 함수가 동기적 I/O이기 때문이며, 동기적이라는 말은 작업간의 순서가 있다는 말이다.

한 프로세스 내에서 현재 작업이 완료되지 않았는데 다음 작업을 수행할 수가 없다는 의미이기에 스레드가 다음 작업을 수행할 수 없이 Blocking 상태가 되어버리는 것이다.

그런데, Nginx는 비동기 Event-Driven이라고 했는데?..

비동기라는 말은, I/O요청이 완료될 때까지 기다리지 않고, 다른 작업을 먼저 처리할 수 있다는 것인데..

요청을 이벤트 큐에 등록하고 이벤트가 완료되면 적절한 콜백함수가 호출되는 방식일텐데..

Nginx의 워커 프로세스는 단일 스레드로 동작하며, 이벤트 큐에 있는 작업을 하나씩 처리하게 된다.

워커 프로세스는 epoll과 같은 고성능 이벤트 감시 메커니즘을 사용하여, 비동기적으로 이벤트를 감지하고 처리한다.

**문제는 Disk I/O 작업이 들어올 때 발생한다**

Disk I/O 작업은 read() 또는 write() 호출 시 완료될 때까지 차단되며, CPU는 대기하지 않더라도 워커 프로세스 자체가 해당 작업을 기다리게 된다.

이로 인해, 워커 프로세스는 이벤트 큐에 있는 다른 작업을 처리하지 못하게 된다..!!

그래서 Nginx에서는 이렇게 오래걸리는 Disk I/O 작업의 경우,

`Thread Pool`을 도입하였다.

Thread Pool은 생성비용이 비싼 스레드를 필요할 때 편하게 늘리기 위해 스레드를 미리 만들어놓고 필요한 작업에게 할당해주는 개념이다.

각 워커 프로세스들은 Disk I/O 처럼 오래 걸리는 작업이 감지되면 해당 작업만 처리하는 역할을 하는 Thread Pool에게 처리를 위임하고 다른 이벤트를 처리한다.

**WAS(Web Application Server)**

웹 서버와 비슷하지만, 클라이언트의 요청에따라 동적인 데이터를 생성하여 제공

사용자의 요청에 따라 변화하는 데이터나, 비즈니스 로직을 처리하는 역할

서버 측 스크립팅 언어를 사용 => DB와의 연동 / 비즈니스 로직을 실행

예시 : Tomcat

실제 서비스에서는 웹 서버와 WAS를 함께 사용하는 경우가 많다...

사실 웹서버가 할 수 있는 일을 WAS도 전부 가능하지만, 굳이 따로 두는 이유는..

WAS는 DB조회 및 비즈니스 로직을 처리하는 데 집중해야하기 때문이다.
따라서 단순한 정적 콘텐츠는 웹 서버에게 맡기며 기능을 분리해 서버 부하를 방지해줘야 한다.

![](https://velog.velcdn.com/images/sujipark2009/post/cb993d92-c3b6-4bd7-afd0-e51c95f7de0e/image.png)

#### REST API에 대해 설명해 주세요

---

**REST(REpresentational State Transfer)** ?

서비스의 아키텍처 스타일 중 하나.

네트워크 아키텍처 원리의 모음

**자원(resource)을 이름(representation)으로 구분하여 해당 자원의 상태를 주고받는 모든 것을 의미.**

기본적으로 웹의 기존 기술과 HTTP 프로토콜을 그대로 활용하기 때문에, 웹의 장점을 최대한 활용할 수 있는 아키텍처.

HTTP 표준에 따르고 URI를 통해 자원을 명시

**HTTP 메서드(GET,POST,PUT,DELETE 등)을 통해 해당 자원에 대한 CRUD 연산을 적용하는 방식**

**REST API**

REST 아키텍처를 따르는 API.

API(Application Programming Interface)는 소프트웨어 간 상호작용을 가능하게 해주는 표준

REST 아키텍처를 통해 설계된 API는 RESTful API

#### RESTful api란 무엇인가요?

---

REST 아키텍처 스타일을 따르는 api

클라이언트와 서버 간의 통신을 간소화하고, 확장성과 유지 보수성을 향상

제약조건은 다음과 같다.

1. **Client-Server Architecture**

클라이언트와 서버가 서로 독립적으로 분리

2. **Stateless**

각 요청은 서버에 저장된 이전 요청과는 독립적
각 요청이 필요한 모든 정보를 가지고 있어야 함

3. **Cacheable**

클라이언트가 요청의 응답을 캐싱할 수 있어야 한다.

클라이언트의 이후 같은 요청에 대해 응답시간을 줄이고, 서버의 부하를 줄임

4. **Uniform Interface**

URI를 통해 자원을 식별
HTTP 메서드를 통해 자원에 대한 연산을 수행

5. **Layered System**

클라이언트는 일반적으로 최종 서버에 직접 연결되는 것으로 알고 있지만, 중간에 다른 계층(로드 밸런서,캐시 서버 등)이 존재할 수 있음

#### 웹 소켓과 소켓 통신의 차이에 대해 설명해주세요.

---

모두 네트워크 프로토콜의 일부.

서버와 클라이언트 간의 데이터 통신을 가능하게 함

**소켓 통신**

TCP/IP 프로토콜을 기반으로 한 저수준 네트워크 프로그래밍 기법

서버와 클라이언트 간의 양방향 통신이 가능

주로 실시간 데이터 전송이 필요한 상황에서 사용

서버와 클라이언트는 연결이 유지되는 동안에는 데이터를 자유롭게 주고받을 수 있음

**웹소켓**

웹 기반의 통신 프로토콜
HTTP 프로토콜 위에서 작동

웹소켓도 소켓 통신처럼 양방향 통신을 지원

웹 환경에서 더욱 효과적으로 작동하도록 설계

HTTP 핸드쉐이크를 통해 연결을 초기화
이후에는 지속적이고 양방향적인 메시지 교환을 가능

웹 환경에서 실시간 통신을 가능하게 하는 기술

채팅 어플리케이션,멀티플레이어 게임 등에서 널리 활용

둘은 상반되는 개념이 아니다.
다만, 소켓 통신을 기반으로 웹 어플리케이션에서 사용하기 쉬운 형태로 발전시키고 표준 프로토콜로 정의하여 웹소켓을 사용한다.

우선 둘은 추상화 정도가 다르다. TCP Socket은 저수준, 웹소켓은 추상화되어있다.

Protocol 또한 다른데, TCP Socket은 4계층에서 동작하지만, Websocket은 HTTP에 기반하므로 7계층에서 동작한다.

Data 전송방법은, TCP socket의 경우 바이트 스트림을 통한 데이터 전송을 사용하지만 Websocket은 구조화된 메시지 형식의 데이터를 다루게 된다.

#### 소켓과 포트의 차이가 무엇인가요?

---

**소켓**

네트워크 통신을 위한 인터페이스를 제공

컴퓨터의 응용 프로그램들이 네트워크를 통해 데이터를 주고받기 위해서는 소켓을 통해야 한다.

응용 프로그램과 네트워크 프로토콜 간의 통신을 가능하게 한다.

**포트**

포트는 특정 프로세스에 데이터를 전달하기 위해 사용되는 숫자로 된 식별자이다.

한 컴퓨터 내에서 여러 응용 프로그램이 네트워크 통신을 하려면, 각각의 프로그램을 식별할 수 있는 방법이 필요한데 이때 사용되는것이 포트번호이다.

HTTP 통신을 위한 기본 포트는 80이고 HTTPS는 443

#### 여러 소켓이 있다고 할 때, 그 소켓의 포트 번호는 모두 다른가요?

---

일반적으로 시스템 내에서 서로 다른 소켓이 동일한 포트 번호를 가지고 있을 수 없음

=> 포트 번호가 네트워크 통신을 수행하는 프로세스를 구분하기 위한 식별자 역할을 하기 때문이다ㅏ.

예를들어 웹 서버가 HTTP 요청을 받기 위해 포트 80을 사용한다면, 그 시스템 내에서 다른 프로세스는 포트 80을 사용할 수 없다.

만약 다른 프로세스가 같은 포트를 사용하려고 시도하면, "Address already in use"와 같은 오류 메시지가 발생한다.

그러나 같은 포트 번호를 사용하는 서로 다른 소켓이 있을 수 있는 경우도 있다.

소켓이 서로 다른 네트워크 인터페이스 혹은 서로 다른 IP 주소를 사용하고 있을 때

서로 다른 IP주소의 조합과 포트 번호를 통해 소켓을 구별할 수 있음

따라서 포트 번호는 네트워크 주소와 함께 사용되어 특정 네트워크 연결에 대한 유일한 식별자를 형성

이것도 좀 정리를 해보자.

**Q.소켓에는 동일한 포트번호를 할당 가능한가 ?**

불가능하다.

왜냐하면, 포트 번호는 하나의 운영체제 안에서 소켓을 구분하는 목적으로 사용되기 때문에 하나의 운영체제 내에서 동일한 포트번호를 둘 이상의 소켓에 할당할 수 없기 때문이다.

데이터 전송 시에 동일한 포트번호를 사용하는 소켓이 둘 이상 존재하면 데이터를 어느 소켓을 통해 송신을 할 지 운영체제로서는 알 길이 없기 때문이다.

OK 여기까지 이해함

그런데.. 이런 말도 있다

**일반적으로 동일한 포트에서 수십 수만개의 소켓을 생성할 수 있기 때문에 서버에서는 매우 많은 수의 클라이언트가 접속 하더라도 손쉽게 처리 가능하다**

이 말도 맞는 말 같다.
그런데 여기서 인지부조화가 온다.

**그래서 동일한 포트에 소켓을 여러개 생성할 수 있다는 것인가?**

...

결론적으로 말하면 **둘 다 맞는말이다**

포트번호당 하나의 소켓만 생성 가능하다는 것은 (서버IP,서버Port,클라이언트IP,클라이언트Port) 이 4가지가 전부 같은 경우를 의미하는 것이고,

서버에서 동일한 포트의 소켓을 여러개 생성하여 클라이언트와 통신을 한다는 것은 통신을 하는 클라이언트 측 IP,Port가 다르기 때문에 뒤의 두 값이 다른 경우를 의미한다.

gpt의 예시를 보면..

**리스닝 소켓**:

- 동일한 포트 번호로 두 개 이상의 리스닝 소켓을 열 수 없습니다.
- 운영 체제는 리스닝 소켓을 (서버 IP, 서버 포트)로 식별하기 때문입니다.

**연결 소켓**:

- 리스닝 소켓에서 클라이언트의 연결 요청을 수락하면, 동일한 포트 번호를 사용하는 여러 연결 소켓이 생성될 수 있습니다.
- 이때, 각 소켓은 (클라이언트 IP, 클라이언트 포트) 값이 다르기 때문에 고유하게 식별됩니다.

HTTP 서버 (포트 80)

1. 서버는 포트 80에서 리스닝 소켓을 엽니다.
2. 클라이언트 A가 서버에 연결 요청 → accept() 호출 후 새로운 소켓이 생성.

- (서버IP, 80, 클라이언트A_IP, 클라이언트A_Port)

3. 클라이언트 B가 서버에 연결 요청 → 또 다른 소켓 생성.

- (서버IP, 80, 클라이언트B_IP, 클라이언트B_Port)

4. 클라이언트 A와 B는 **동일한 서버 포트(80)**를 사용하지만, 서로 다른 소켓으로 통신합니다.

따라서, **"서버의 동일한 포트 번호에서 여러 소켓 생성이 가능하다"**는 말은 올바른 설명입니다.

참고로 TCP와 UDP는 독립적인 프로토콜 이므로, 같은 포트번호를 동시에 사용할 수 있다.

따라서 TCP 80번 포트와 UDP 80번 포트를 동시에 **리스닝 포트**로 사용할 수 있다.

#### 서로 다른 소켓이 동일한 포트 번호를 사용할 수 없고, HTTP 통신을 위한 기본 포트가 80이라면, 여러 어플리케이션에서 HTTP 통신을 진행중인 경우 어떻게 구분하고 순서가 보장되나요?

---

네트워크 통신에서 한 컴퓨터 내에서 여러 프로세스나 어플리케이션이 동일한 포트 번호를 사용하지 못하는 것은 사실이다.

이는 "동일한 IP주소와 동일한 프로토콜" 을 사용하는 프로세스나 어플리케이션에만 해당한다.

HTTP 통신을 위한 기본 포트가 80이라는 것은, 웹 서버가 클라이언트로부터의 연결을 기다리는 리스닝 포트가 80이라는 의미이다.

여러 클라이언트 어플리케이션이 웹 서버에 연결을 시도할 때 각각의 연결은 고유한 소켓을 생성하게 된다.

소켓의 구성은 (서버의 IP,서버의 PORT,클라이언트 IP,클라이언트 임시 PORT)로 구성이 된다.

각각의 클라이언트 연결은 고유한 클라이언트 IP와 임시 포트 번호를 가진다.

따라서 웹 서버는 이를 통해 각각의 연결을 구분할 수 있다.

#### 동일한 클라이언트에서 여러 어플리케이션을 사용하는 경우, 기본 HTTP 포트가 80인데 어떻게 구분하나요?

---

서버 측에서는 일반적으로 HTTP 서비스를 위해 80번 포트를 사용한다.

클라이언트 측에서는 운영체제에 의해 임시적으로 할당받은 고유한 포트 번호를 사용하게 되는데, 일반적으로 1024 이상의 번호를 할당받게 된다.

여러 어플리케이션은 같은 클라이언트 IP를 가지고 있으나, 각각 다른 포트 번호를 가지고 있다.

따라서 서버측에서는 이 클라이언트 IP와 포트 번호의 조합을 통해 각각의 연결을 구분할 수 있다.

### API Gateway란 무엇인가요?

---

안써봐서유기

#### webRTC란 무엇인가요

---

**WebRTC(Web Real-Time Communication)** 란, WebApp 및 사이트들이 별도의 소프트웨어 없이 음성,영상,텍스트,파일 같은 데이터를 브라우저끼리 주고받을 수 있게 만든 기술이다

쉽게 말해서 `다양한 플랫폼에서 가능한 실시간 커뮤니케이션 기술`이라고 생각하면 된다.

서버없이 P2P(Peer To Peer)로 연결되어 데이터를 주고받을 수 있다.
단, P2P로 연결되기 위한 과정에서 중계서버(시그널링 서버)는 필요하다.

WebRTC를 뒷받침하는 기술은 웹 표준으로 구현되며 모든 주요 브라우저에서 일반 자바스크립트 API로 제공된다.

**시그널링(Signaling)**

P2P로 연결하기 위해서는 중계서버가 필요한데, 중계서버를 통해서 서로를 찾고 연결을 시작하는 과정을 바로 시그널링 이라고 한다.

이 단계에서는 SDP(Session Description Protocol)을 생성하는데,

SDP는 무엇으로 소통할지, 어떻게 소통할지 에 대한 정보가 담겨있는 양식이다.

즉, 나는 이런 비디오/오디오를 지원해!! 라고 알려주는 자기소개서와 같다고 보면 된다.

더 자세하게는 세션정보,네트워크 정보,미디어의 종류,코덱,해상도,스트림의 속성 등 내용이 담겨있다.

그래서 연결을 시작하는 쪽에서는 `Offer SDP`를 생성하고, 받는 쪽에서는 `Answer SDP`로 응답한다.

이 정보들을 통해서 양쪽 피어가 지원하는 코덱,해상도 등을 협상하여 최적의 설정을 결정하게 되는 것이다.

**ICE(Interactive Connectivity Establishment)**

위에서 자기소개(SDP)를 했으니.. 이제 어떻게 연락을 할 지 정해야한다.

ICE는 `나한테 연락하려면 이 주소로 연락해` 라고 알려주는 과정이다.

즉, 두 Peer 간의 `최적의 통신 경로` 를 찾기위해 사용되는 프레임워크이다.

- 연결 후보 수집 : 가능한 모든 연결 방법(후보)을 찾아낸다.
- 연결성 검사 : 각 후보에 대해 실제 연결 가능 여부를 테스트한다.
- 우선순위 결정 : 가장 효율적인 연결 경로(지연시간이 가장 짧은 연결을 선호)를 선택한다.

- NAT 및 방화벽 통과 : 다양한 네트워크 환경에서의 연결을 가능하게 한다.

SDP교환 후, 각 클라이언트는 자신이 사용할 수 있는 네트워크 경로(ICE Candidate)를 생성하게 되고 ICE Candidate에는 3가지 유형이 있다.

1. **Host Candidate** : 클라이언트의 로컬 네트워크 주소(IP)

2. **Server Reflexive Candidate** : STUN 서버를 통해 얻은 공용 IP 주소

3. **Relay Candidate** : TURN 서버를 통해 생성된 중계용 IP 주소

ICE과정은 다음과 같은데,

1. STUN 서버와의 상호작용

- 클라이언트는 STUN 서버에 요청을 보내 자신의 공용 IP주소와 포트를 확인한다
- NAT 환경에서는 이 과정이 필수이다.

2. TURN 서버 사용(필요 시)

- P2P 연결이 불가능한 경우, TURN 서버를 통해 중계용 IP 주소를 생성한다.

3. ICE Candidate 교환

- 각 클라이언트는 생성된 ICE Candidate를 시그널링 서버를 통해 상대방에게 전달한다

4. P2P 연결 수립

클라이언트는 교환한 SDP와 ICE Candidate를 바탕으로 P2P 연결을 설정한다.

4-1. ICE Candidate 테스트

- 클라이언트는 상대방으로부터 받은 ICE Candidate를 바탕으로 연결 테스트를 실행한다.
- 이 과정에서 가장 효율적인 경로를 선택한다(ex: Host -> Server Reflexive -> Relay 순으로 시도)

4-2. DTLS(Datagram Transport Layer Security)

- 연결이 성공하면 클라이언트 간 보안 채널을 설정하기 위해 DTLS 핸드셰이크를 실행한다.

4-3. SRTP 세션 시작

- 미디어 데이터(오디오/비디오)를 전송하기 위해 SRTP(Secure Real-Time Transport Protocol)를 사용하여 암호화된 데이터 스트림을 전송한다.

공통 떄 사용했던 Openvidu를 생각해보면..

클라이언트에서 Openvidu Session에 입장하기 위해 Session ID를 생성하여 Openvidu Token 서버로 전달하였다.

해당 SessionID로 토큰 서버가 Openvidu 프레임워크를 이용하여 방을 생성하였고, 그 방에 입장하기 위한 Token을 클라이언트에게 전달해주었다.

사실 저 Openvidu가 방을 생성하는 과정에는, 저런 시그널링 과정이 다 들어가 있는 것이다.

P2P연결을 하기 전에는 시그널링 서버와는 WebSocket통신을 해서 위에서 설명한 시그널링 과정(SDP,ICE Candidate,STUN/TURN서버 ..)을 거쳐야 하는데 이 과정을 Openvidu가 다 해주는 것이다!!

Openvidu의 역할은, 시그널링을 추상화하고 세션관리 및 보안관리(토큰발급)을 해주는 것이다.

그래서 P2P 끼리 연결이 된 이후에는, 중간 서버를 거쳐야 하는 WebSocket과는 달리 브라우저 간 직접연결을 하기 때문에 더 빠르다!!

WebSocket또한 미디어 전송을 할 수 있으나, 다음과 같은 차이가 있다

![](https://velog.velcdn.com/images/sujipark2009/post/626d0608-c99b-4574-9a3a-ea8cb28a49ec/image.png)

**1. P2P 연결로 대역폭 절약**

웹소켓은 클라이언트-서버 간 연결만 지원하며, 모든 데이터를 서버를 경유하여 전송해야 한다.

그래서 영상,오디오 데이터를 전송할 경우 서버에 큰 부하가 발생한다.

webRTC는 P2P연결을 통해 클라이언트 간 데이터를 직접 전송하므로 서버 부하를 크게 줄일 수 있다.

**2. 낮은 지연 시간**

webRTC는 SRTP(Secure Real-Time Transport Protocol)과 같은 실시간 데이터 전송 프로토콜을 사용한다.

- **RTCPeerConnection**을 통해 미디어 데이터를 바로 P2P로 전송한다.

반면, webSocket은 **TCP 기반**으로 동작하며, 데이터가 순차적으로 전달되므로 지연시간이 증가할 수 있다.

- 오디오와 비디오에는 UDP 기반 프로토콜이 적합하며, WebRTC는 이를 활용한다.

이외에도.. WebRTC는 네트워크 상태에 따라 **비디오 해상도,프레임 레이트,오디오 품질**을 동적으로 조정한다고 한다.

webSocket보다 webRTC가 영상데이터 쪽에 더 특화되도록 설계되었다고 한다~

**STUN 서버**

STUN(Session Traversal Utilities for NAT)은 우리의 컴퓨터가 `나 지금 어디에 있는거야?` 라고 물어볼 때 대답해주는 친절한 서버라고 생각하면 된다.

NAT환경에서 사설 IP를 별도로 가지고 있기 때문에 P2P 통신이 불가능한데,
따라서 클라이언트는 자신의 Public IP를 확인하기 위해 STUN서버로 요청을 보내고 서버로부터 자신의 Public IP를 받게된다.

**TURN 서버**

TURN(Traversal Using Relays aroudn NAT)은 마치 `너희 둘이 만나기 어려우면, 내가 중간엥서 메세지를 전달해줄게` 라는 친절한 중계자ㅏ이다.

TURN서버는 STRN서버로 해결할 수 없는 상황에서 사용된다.
일부 NAT 구성에서는 외부 서버와의 직접적인 P2P연결을 허용하지 않는다.

그래서 TURN 서버의 특징으로는 완벽한 중계 서비스를 제공하지만, 상대적으로 느리고 리소스를 많이 사용한다.

그래서 WebRTC는 보통

1. 먼저 STUN으로 해볼까? 잘되네? 이걸로 ㄱㄱ
2. STUN이 안되네? 그럼 TURN에게 부탁 ㄱㄱ

즉, 1번을 머너저 시도하고 안되면 2번으로 시도하게 된다

**P2P 연결의 한계**

![](https://velog.velcdn.com/images/sujipark2009/post/5685a393-45e2-4def-81fd-5ed2a49f27aa/image.png)

P2P 연결은 webRTC의 기본이지만, 참가자가 많아지면 과부하가 오기 시작한다.
만약 N명이 참가하면 N(N-1)/2 개의 연결이 필요하게 된다.
참가자가 10명이면 45개의 연결이 생성되게 된다.

즉, 이렇게 되면 CPU에 과부하가 올 가능성이 매우 높다.
대신 보통 P2P 구현은 서버에 별도의 부하가 없고, Peer간의 직접적인 연결로 실시간성이 보장된다.
그래서 매우 적은 사용자들만 필요로 할 때 사용하게 되는 구현방식이다.

효율적으로 처리하는 방식은 다음과 같다.

\*\*SFU(Selective Forwarding Unit)

![](https://velog.velcdn.com/images/sujipark2009/post/3dcfbfb0-6c59-4a2b-9ef7-860294bbc882/image.png)

SFU는 `영리한 미디어 라우터`라고 생각하면 된다.

미디어간 트래픽을 중계하는 중앙 서버이며 서버와 클라이언트 간의 Peer를 연결한다

- 각 참가자로부터 하나의 스트림만 받아서 필요한 곳에 전달한다
- 참가자들의 대역폭과 성능에 따라 영상 품질을 조절할 수 있다.
- 확장성이 뛰어나고, 지연 시간도 비교적 짧다

작동방식은 다음과 같다

1. A가 영상을 SFU로 한번만 전송한다
2. SFU가 이 영상을 B,C,D에게 각각 전달한다
3. A는 업로드를 한번만 하면 된다.

그래서 1:N 형식 또는 소규모 N:M 형식의 실시간 스트리밍이 필요한 상황에서 주로 채택한다.

Openvidu 에서는 이 SFU방식을 채택하였으며, 이걸 해주는 중계서버는 **Kurento Media Server** 였다

**MCU(Multipoint Control Unit)**

![](https://velog.velcdn.com/images/sujipark2009/post/91a34801-8f65-44ab-8cd3-3cab45a4e6b7/image.png)

MCU는 영상을 받아서 섞은 다음에 `하나의 스트림`으로 만들어 보내게 된다.

MCU방식은 클라이언트에 부담이 가장 적고, 일관된 경험을 제공하지만 서버 리소스를 가장 많이 사용한다

그리고 최대 단점으로는 webRTC의 최대 장점인 `실시간성이 보장되지 않을 수 있다`

작동방식은 다음과 같다

1. A,B,C가 각자의 영상을 MCU로 보낸다
2. MCU가 이 영상들을 `하나의 스트림`으로 만든다
3. 그리고 이 통합된 스트림을 모든 참가자에게 보내주게 된다.

### SOP 정책에 대해 설명해 주세요.

---

**출처(Origin)란 ?**

우리가 어떤 사이트를 접속할 때 인터넷 주소창에 우리는 URL 이라는 문자열을 통해 접근하게 된다.

이처럼 URL은 https://domain.com:3000/user?query=name?page=1 과 같이 하나의 문자열 같지만, 사실은 다음과 같이 여러개의 구성 요소로 이루어져 있다.

![](https://velog.velcdn.com/images/sujipark2009/post/1f418bfb-9196-49be-b166-28599e4fb47f/image.png)

- Protocol(Scheme) : http,https
- Host : 사이트 도메인
- Port : 포트번호
- Path : 사이트 내부 경로
- Query string : 요청의 key와 value값
- Fragment : 해시태그

여기서, Origin : Protocol + Host + Port를 합친것을 의미한다

즉, **출처(Origin)라는 것은 Protocol 과 Host 그리고 Port까지 모두 합친 URL**을 의미한다고 보면 된다.

**동일 출처 정책(Same-Origin Policy)**

SOP(Same Origin Policy) 정책은 단어 그대로 **동일한 출처에 대한 정책**을 말한다.

그리고 이 SOP 정책은 `동일한 출처에서만 리소스를 공유할 수 있다` 라는 법률을 가지고 있다.

즉, 동일 출처(Same-Origin) 서버에 있는 리소스는 자유로이 가져올 수 있지만, 다른 출처(Cross-Origin) 서버에 있는 이미지나 유튜브 영상 같은 리소스는 상호작용이 불가능하다는 말이다.

![](https://velog.velcdn.com/images/sujipark2009/post/3fd1e057-0635-43a7-b388-9db7ea203276/image.png)

**동일 출처 정책이 필요한 이유**

그렇다면 동일 출처가 아닌 경우 접근을 차단하는 이유는 뭘까?

사실 출처가 다른 두 어플리케이션이 자유로이 소통할 수 있는 환경은 꽤 위험한 환경이다. 만약 제약이 없다면, 해커가 CSRF(Cross-Site Request Forgery)나 XSS(Cross-Site Scripting) 등의 방법을 이용해서 우리가 만든 어플리케이션에서 해커가 심어놓은 코드가 실행하여 개인 정보를 가로챌 수 있다.

![](https://velog.velcdn.com/images/sujipark2009/post/856b057d-0025-48f1-9ae6-72fb8c15fd65/image.png)

1. 사용자가 악성 사이트에 접속한다.
2. 이때 해커가 몰래 심어놓은 악의적인 자바스크립트가 실행되어, 사용자가 모르는 사이에 어느 포털 사이트에 요청을 보낸다.

3. 그럼 포털 사이트에서 해당 브라우저의 쿠키를 이용하여 로그인을 하는 등 상호작용에 따른 개인정보 응답 값을 받은 뒤, 사이트에서 해커 서버로 재차 보낸다.
4. 이외에도 사용자가 접속중인 내부망의 아이피와 포트를 가져오거나, 해커가 사용자 브라우저를 프록시처럼 악용할 수도 있다.

**같은 출처와 다른 출처 구분 기준**

출처(Origin)의 동일함은 두 URL의 구성 요소 중 `Protocol(Scheme),Host,Port`이 3가지만 동일하다면 동일 출처로 판단한다.

**출처 비교와 차단은 브라우저가 한다**

출처를 비교하는 로직은 서버에 구현된 스펙이 아닌 **브라우저에 구현된 스펙**이다.

![](https://velog.velcdn.com/images/sujipark2009/post/38099cd4-15b8-4c38-8d18-00eaee78de26/image.png)

사실 서버는 리소스 요청에 의한 응답은 말끔히 해주었다. 잘못이 없는 것이다.

하지만 브라우저가 이 응답을 분석해서 동일 출처가 아니면, 에러를 내뿜는 것이다.

그래서 브라우저에는 에러가 뜨지만, 정작 서버 쪽에서는 정상적으로 응답을 했다고 하기 때문에 난항을 겪는 것이다.

즉, 응답 데이터는 멀쩡하지만 브라우저 단에서 받을 수 없도록 차단을 한 것이다.

**그럼 죄다 차단하면 인터넷이 되는가?**

하지만 인터넷은 여러 사람들에게 오픈된 환경이고, 이런 환경에서 웹페이지에서 다른 출처에 있는 리소스를 가져와 사용하는 일은 매우 흔한 일이라 무턱대고 막을 수는 없는 일이다.

![](https://velog.velcdn.com/images/sujipark2009/post/84246a53-04c5-414c-a6c7-7bed2be805be/image.png)

그래서 몇 가지 **예외 조항**을 두고 다른 출처의 리소스 요청이라도 이 조항에 해당할 경우에는 허용하기로 했는데, 그 중 하나가 바로 `CORS 정책을 지킨 리소스 요청`이다.

#### CORS 정책이 무엇인가요?

---

교차 출처 리소스 공유(Cross Origin Resource Sharing,CORS)는 단어 그대로 **다른 출처의 리소스 공유에 대한 허용/비허용 정책**이다.

아무리 보안이 중요하지만, 개발을 하다 보면 기능상 어쩔 수 없이 다른 출처 간의 상호작용을 해야하는 케이스도 있으며, 또한 실무적으로 다른 회사의 서버 API를 이용해야 하는 상황도 존재한다.

따라서 이와 같은 예외 사항을 두기 위해 CORS 정책을 허용하는 리소스에 한해 다른 출처라도 받아들인다는 것이다.

**CORS는 사실 해결책이었다**

아 시XX발 또 CORS떴네?

이건 사실 브라우저의 SOP 정책에 따라 다른 출처의 리소스를 차단하면서 발생된 에러이며, CORS는 다른 출처의 리소스를 얻기위한 해결 방안 이었던 것이다.

요약하자면 `SOP 정책을 위반해도 CORS 정책에 따르면 다른 출처의 리소스라도 허용`한다는 뜻이다.

**브라우저의 CORS 기본 동작**

1. **클라이언트에서 HTTP 요청의 헤더에 Origin을 담아 전달**

- 기본적으로 웹은 HTTP 프로토콜을 통해 서버에 요청을 보내게 되는데,
- 이때 브라우저는 요청 헤더에 Origin 이라는 필드에 출처를 함께 담아 보내게 된다.

![](https://velog.velcdn.com/images/sujipark2009/post/e2e4080c-8024-4de8-b1a1-b2dbaea4e46b/image.png)

2. **서버는 응답헤더에 Access-Control-Allow-Origin을 담아 클라이언트로 전달한다.**

- 이후 서버가 이 요청에 대한 응답을 할 때 응답 헤더에 `Access-Control-Allow-Origin` 이라는 필드를 추가하고 값으로 **이 리소스를 접근하는 것이 허용된 출처 url**을 내려보낸다.

![](https://velog.velcdn.com/images/sujipark2009/post/1afdf116-2e28-4399-8c4c-aac5c4159720/image.png)

3. **클라이언트에서 Origin과 서버가 보내준 Access-Control-Allow-Origin을 비교한다.**

- 이후 응답을 받은 브라우저는 자신이 보냈던 요청의 Origin과 서버가 보내준 응답의 Access-Control-Allow-Origin을 비교해본 후 차단할지 말지를 결정한다.
- 만약 유효하지 않다면 그 응답을 사용하지 않고 버린다.(CORS 에러)
- 위의 경우에는 둘다 http://localhost:3000이기 때문에 유효하니 다른 출처의 리소스를 문제없이 가져오게 된다.

**결국 CORS 해결책은 서버의 허용이 필요**

위의 브라우저의 CORS 동작 과정을 살펴보니, 결론은 **서버에서** `Access-Control-Allow-Origin` 헤더에 허용할 출처를 기재해서 클라이언트에 응답하면 되는 것이었다.

> 그렇다면 클라이언트에서 미리 자바스크립트로 origin 헤더값을 위조하면 되지 않을까 싶지만, 브라우저에서 이를 감지하여 차단하기 때문에 불가능하다
> ![](https://velog.velcdn.com/images/sujipark2009/post/b4d26cf9-b655-45de-8efe-87d1b596acc4/image.png)

#### Preflight에 대해 설명해주세요

---

**예비 요청(Preflight Request)**

브라우저는 요청을 보낼때 한번에 바로 보내지않고, 먼저 **예비 요청**을 보내 서버와 잘 통신이 되는지 확인한 후 **본 요청**을 보낸다.

즉, 예비 요청의 역할은 본 요청을 보내기 전에 브라우저 스스로 안전한 요청인지 미리 확인하는 것이다.

이때 브라우저가 예비요청을 보내는 것을 **Preflight**라고 부르며, 이 예비요청의 HTTP 메서드를 GET이나 POST가 아닌 `OPTIONS` 라는 요청이 사용된다는 것이 특징이다.

![](https://velog.velcdn.com/images/sujipark2009/post/e00aeea7-2f0c-4d4b-b426-5e65bf154e55/image.png)

1. 자바스크립트의 `fetch()` 메서드를 통해 리소스를 받아오려고 한다.

2. 브라우저는 서버로 HTTP OPTIONS 메서드로 예비 요청(Preflight)을 먼저 보낸다.

- Origin 헤더에 자신의 출처를 넣는다.
- Access-Control-Request-Method 헤더에 실제 요청에 사용할 메서드를 설정한다.
- Access-Control-Request-Headers 헤더에 실제 요청에 사용할 헤더들을 설정한다.

3. 서버는 이 예비요청에 대한 응답으로 어떤 것을 허용하고 어떤 것을 금지하고 있는지에 대한 헤더 정보를 담아서 브라우저로 보내준다.

- Access-Control-Allow-Origin 헤더에 허용되는 Origin들의 목록을 설정한다.
- Access-Control-Allow-Methods 헤더에 허용되는 메서드들의 목록을 설정한다.
- Access-Control-Allow-Headers 헤더에 허용되는 헤더들의 목록을 설정한다.
- Access-Control-Max-Age 헤더에 해당 예비 요청이 브라우저에 캐시 될 수 있는 시간을 초 단위로 설정한다.

4. 이후 브라우저는 보낸 요청과 서버가 응답해준 정책을 비교하여, 해당 요청이 안전한지 확인하고 본 요청을 보내게 된다.

5. 서버가 본 요청에 대한 응답ㅇ블 하면 최종적으로 이 응답 데이터를 넘겨준다.

**예비 요청의 문제점과 캐싱**

요청을 보내기 전에 OPTIONS 메서드로 예비 요청을 보내 보안을 강화하는 목적의 취지는 좋으나, 그러나 결국은 실제 요청에 걸리는 시간이 늘어나게 되어 어플리케이션 성능에 영향을 미치는 크나큰 단점이 있다.

특히 수행하는 API 호출 수가 많으면 많을 수록 예비 요청으로 인해 서버 요청을 배로 보내게 되니 비용적인 측면에서 비효율적일 수 있다.

따라서 `브라우저 캐시`를 이용해 `Access-Control-Max-Age` 헤더에 캐시될 시간을 명시해주면, 이 Preflight 요청을 캐싱 시켜 최적화를 시켜줄 수 있다.

![](https://velog.velcdn.com/images/sujipark2009/post/5ae7df61-e15d-494b-acb5-8f41e13fb564/image.png)

> 예비 요청 캐싱 기간에 대해서는, 파이퍼폭스의 경우 24시간까지 / 크롬은 2시간이 최대라고 함

![](https://velog.velcdn.com/images/sujipark2009/post/651e2e27-efe0-489f-8e2a-2e5d473a7778/image.png)

예비 요청 캐시는 다른 캐싱 메커니즘과 유사하게 작동한다.

1. 브라우저는 예비 요청을 할 때마다, 먼저 Preflight 캐시를 확인하여 해당 요청에 대한 응답이 있는지 확인한다.

2. 만일 응답이 캐싱 되어있지 않다면 서버에 예비 요청을 보내 인증 절차를 밟는다.

3. 만일 서버로 부터 Access-Control-Max-Age 응답 헤더를 받는다면 그 기간 동안 브라우저 캐시에 결과를 저장한다

4. 다시 요청을 보내고 만일 응답이 캐싱 되어 있다면, 예비 요청을 서버로 보내지 않고 대신 캐시된 응답을 사용한다.

**단순 요청(Simple Request)**

단순 요청은 말 그대로 **예비 요청(Preflight)을 생략하고 바로 서버에 직행으로 본 요청**을 보낸 후, 서버가 이에 대한 응답의 헤더에 Access-Control-Allow-Origin 헤더를 보내주면 브라우저가 CORS 정책 위반 여부를 검사하는 방식이다.

![](https://velog.velcdn.com/images/sujipark2009/post/92e35b28-73db-4805-9918-3eaf97d1979f/image.png)

다만, 심플한 만큼 특정 조건을 만족하는 경우에만 예비 요청을 생략할 수 있다.

대표적으로 아래 **3가지 경우를 만족** 할때 만 가능하다.

1. 요청의 메서드는 `GET`,`HEAD`,`POST` 중 하나여야 한다.

2. `Accept`,`Accept-Language`,`Content-Language`,`Content-Type`,`DPR`,`Downlink`,`Save-Data`,`Viewport-Width`,`Width` 헤더일 경우에만 적용된다.

3. Content-Type 헤더가 `application/x-www-form-urlencoded`,`multipart/form-data`,`text/plain` 중 하나여야 한다. 아닐 겨웅 예비 요청으로 동작된다.

이처럼 다소 까다로운 조건들이 많기 때문에, 위 조건을 모두 만족되어 단순 요청이 일어나는 상황은 드물다고 보면 된다.

왜냐하면 대부분의 HTTP API 요청은 `text/xml` 이나 `application/json` 으로 통신하기 때문에 3번째 Content-Type이 위반되기 때문이다.

**그냥 대부분의 API 요청은 예비 요청으로 이루어진다 라고 이해 ㄱㄱ**

**인증된 요청(Credentialed Request)**

인증된 요청은 클라이언트에서 서버에게 **자격 인증 정보(Credential)** 를 실어 요청할때 사용되는 요청이다.

여기서 말하는 **자격 인증 정보** 란 세션 ID가 저장되어있는 `쿠키(Cookie` 혹은 `Authorization 헤더`에 설정하는 `토큰` 값 등을 일컫는다.

즉, 클라이언트에서 일반적인 JSON 데이터 외에도 쿠키 같은 인증 정보를 포함해서 다른 출처의 서버로 전달할때 CORS의 세가지 요청 중 하나인 인증된 요청으로 동작된다는 말이며, 이는 기존의 단순 요청이나 예비 요청과는 살짝 다른 인증 형태로 통신하게 된다.

1. **클라이언트에서 인증 정보를 보내도록 설정하기**

기본적으로 브라우저가 제공하는 요청 API 들은 별도의 옵션 없이 브라우저의 쿠키와 같은 인증과 관련된 데이터를 함부로 요청 데이터에 담지 않도록 되어있다.

이때 요청에 인증과 관련된 정보를 담을 수 있게 해주는 옵션이 바로 `credentials` 옵션이다.

이 옵션에는 3가지 값을 사용할 수 있으며, 각 값들이 가지는 의미는 다음과 같다.

![](https://velog.velcdn.com/images/sujipark2009/post/1af5f75c-7223-4a82-a98c-0d9081859ee6/image.png)

만일 이러한 별도의 설정을 해주지 않으면 쿠키 등의 인증 정보는 절대로 자동으로 서버에게 전송되지 않는다.

```
// axios 라이브러리
axios.post('https://example.com:1234/users/login', {
    profile: { username: username, password: password }
}, {
	withCredentials: true // 클라이언트와 서버가 통신할때 쿠키와 같은 인증 정보 값을 공유하겠다는 설정
})
```

2. **서버에서 인증된 요청에 대한 헤더 설정하기**

서버도 마찬가지로 이러한 인증된 요청에 대해 **일반적인 CORS 요청과는 다르게 대응**해줘야 한다.

1. 응답 헤더의 Access-Control-Allow-Credentials 항목을 true로 설정해야 한다.

2. 응답 헤더의 Access-Control-Allow-Origin 의 값에 와일드 카드 문자("\*")는 사용할 수 없다.

3. 응답 헤더의 Access-Control-Allow-Methods 의 값에 와일드 카드 문자("\*")는 사용할 수 없다.

4. 응답 헤더의 Access-Control-Allow-Headers 의 값에 와일드 카드 문자("\*")는 사용할 수 없다.

즉, 응답의 Access-Control-Allow-Origin 헤더가 와일드카드가 아닌 분명한 Origin으로 설정되어야 하고, Access-Control-Allow-Credentials 헤더는 true로 설정되어야 한다는 뜻이다.

그렇지 않으면 브라우저의 CORS 정책에 의해 응답이 거부된다.

![](https://velog.velcdn.com/images/sujipark2009/post/ccb6303b-f634-4104-b57b-2017f1ae59ce/image.png)

![](https://velog.velcdn.com/images/sujipark2009/post/a66b1cd4-8e73-434b-9f01-5ecb0e851624/image.png)

참고로 인증된 요청 역시 예비 요청 처럼 preflight가 먼저 일어난다.
위의 그림에서는 GET 요청이어서 생략이 된 것.

### HTTPS + TLS/SSL

#### 공개키와 대칭키에 대해 설명해 주세요.

---

**대칭키**

암호화 할 때 사용하는 키와 복호활 할 때 사용하는 키가 같은 암호화 방식

속도가 빠르다는 장점이 있으나, 초기에 암호를 주고받는 사람들 간에 대칭 키 전달이 어렵다는 단점이 있다.

키가 유출되면 공격자가 암호의 내용을 복호화가 가능하다.

클라이언트와 서버가 동일한 키를 사용해 암호화/복호화를 진행한다

**공개키란**

비대칭키 암호화 방식이다.

암호화 및 복호화에 공개키와 비밀키의 쌍을 사용하는 암호화 방식이다.

한 주체가 키 쌍을 생성하게되며 공개키는 누구나 접근이 가능하지만 비밀키(개인키)는 본인만 접근가능하다.

키가 노출되어도 비교적 안전하지만 연산 속도가 느리다

2가지 방식이 존재하는데

1. **공개키로 암호화하고 개인키로 복호화**

데이터를 암호화하여 보호할 목적이다.

- A가 공개키와 개인키를 생성하여 공개키를 전달한다
- B는 데이터를 A의 공개키로 암호화하고 A에게 전달한다
- A는 암호화된 데이터를 개인 키로 복호화해서 데이터를 확인할 수 있다.

공개키로 암호화하면 개인키로만 복호화가 가능하기에, 나만 볼 수 있는 정보가 되는 것임

2. **개인 키로 암호화하고, 공개키로 복호화**

디지털 서명/인증의 목적이다

- A가 데이터를 자신의 개인키로 암호화하여 B에게 전달한다
- B는 A의 공개키로 이를 복호화할 수 있다
- A의 공개키로 복호화가 된다면, 이는 A가 서명한 내용임을 알 수 있다.

개인키로 암호화하면 공개키로만 복호화가 가능하고 공개키는 모두에게 공개되어 있기 때문에, 내가 인증한 정보임을 알려 신뢰성을 보장할 수 있다.

공개키 방식의 장점은, 키 분배를 좀 더 안전하게 할 수 있다는 것이다.

단점은, 상대적으로 대칭키 방식에 비해 느린속도이며 이를 해결하기 위해 공개키와 대칭키를 혼합하여 사용한다.

예시로는 RSA,Elgamal이 있다.

#### SSL이란 무엇인가요?

---

SSL(Secure Socket Layer)

TCP/IP 네트워크에서 통신할 때 사용되는 암호화 기반 프로토콜

특징은,

- 패킷이 탈취되어도 데이터를 지킬 수 있다
- SSL 인증서로 도메인의 신뢰성을 검증 가능
- 송/수신 과정에서 암/복호화가 발생하여 속도가 느림

#### SSL과 TLS의 차이는 무엇인가요?

---

TLS : Transport Layer Security
SSL은 TLS의 이전 버전으로, 현재는 SSL 대신에 TLS를 사용한다.

#### CA의 역할은 무엇인가요?

---

CA(Certificate Authority)의 특징은,

- CA기관들은 본인들의 고유한 비밀키를 가지고 있고, 공개키를 전 세계에 배포한다
- 세상 사람들은 암묵적으로 해당 기관을 신뢰 가능한 기관으로 약속했다
- CA기관의 비밀키는 철처하게 보안을 유지해야 한다.

인증서를 관리하는 역할을 하는데,

- 인증서 발급 : CA는 SSL/TLS 인증서를 발급하고 관리한다

- 인증서 신뢰성 확인 :

CA는 인증서 소유자의 정보를 확인하고, 신뢰할 수 있는 제3자의 역할을 한다.
인증서에 서명한 CA의 디지털 서명을 확인한다

- 인증서 폐기 및 갱신

CA는 인증서의 유효기간을 관리하고, 만료된 인증서를 갱신한다
CA는 인증서 폐기 목록을 유지하여 브라우저가 폐기된 인증서를 확인한다

- 인증서 체인 관리

루트 인증서와 중간 인증서를 사용하여 다양한 인증서를 서명

#### SSL 인증서의 내용은 어떻게 구성되나요?

---

내용은,

인증서를 발급한 CA
서비스의 도메인
인증서의 소유자 이름
인증서의 유효 기간

서버측 공개키의 내용, 암호화 방법

지문 => 인증서의 내용을 종합해 해시화한 값

그리고 이 정보는 CA의 비밀키를 통해 암호화 되어 있다.(서명한다고도 함)

### HTTP와 HTTPS의 차이는 무엇인가요?

---

HTTPS는, HyperText Transfer Protocol Over Secure Socket Layer이다.

TLS 프로토콜을 사용하여 HTTP 보안이 강화된 버전의 프로토콜.

#### HTTPS의 동작 과정을 설명해 주세요.

---

요약하면,

공개키 암호화 방식과 대칭키 암호화 방식의 장점을 합쳐 사용한다.
공개키 방식으로 대칭키를 암호화하여 교환하고, 데이터를 대칭키 방식으로 암호화,복호화 하여 교환한다.

TLS Handshake는 클라이언트와 서버가 안전하게 통신하기 위해 필요한 **암호화 방식** 과 **공유키(Session Key)** 를 설정하는 과정이다.

과정을 보면,

1. **TCP Handshake + TLS Handshake**

![업로드중..](blob:https://velog.io/41a572e4-e5ec-460d-88ca-bd631640dd3d)

TCP Handshake : 3way-handshake

TLS Handshake의 과정을 살펴보면..

1-1. **Client Hello**

클라이언트가 서버에 첫 요청을 전송한다.

반환 데이터로는,

- 클라이언트가 서버에게 전송할 데이터
- 클라이언트 측에서 생성한 난수 데이터(세션 키 생성을 위한 랜덤 데이터)
- 클라이언트가 사용 가능한 암호화 방식 => 클라이언트 - 서버 간 암호화 방식 통일을 위해(예: AES,RSA,ECDHE 등)

- 세션 ID => 이전에 이미 Handshaking 기록이 있다면 자원 절약을 위해 기존 세션을 재활용하기 위함

1-2.**Server Hello**

Client Hello에 대한 응답 전송

반환 데이터로는,

- 서버 측에서 생성한 난수 데이터 => 이것은 클라이언트가 생성한 난수 데이터와는 다른 것이다.(세션 키 생성을 위한 서버 측 랜덤 데이터)

- 서버가 선택한 클라이언트의 암호화 방식

- CA로부터 발급받는 SSL인증서(CA의 서명 + 서버의 공개키 + 서버에 대한 정보)

1-3. **Client 인증 확인**

- 클라이언트는 인증서를 복호화하여 인증서가 CA에 의해 발급되었는지 CA 목록에서 확인한다.

Client(브라우저)가 SSL 인증서(CA의*Private_key로*암호화(Hash(Server에*대한*정보)))를 CA의 Public Key로 디코딩해본다.

그러면 **Hash(Server에*대한*정보)**의 값을 얻을 수 있다. 그리고 **Server에*대한*정보**를 Hash화 해서 서로 일치하는 지 체크한다. 이를 통해 SSL 인증서가 위조 됐는 지 여부를 체크할 수 있다.

- Premaster Secret값 생성 => 클라이언트-서버 각각의 난수를 `조합`한다.

- Premaster Secret값을 서버의 공개키로 암호화 후 서버에 전달

이 값은 서버의 개인키로만 복호화가 가능하기에, 탈취되어도 그 내용이 보호된다.

- Master Secret 및 Session Key 생성 => Premaster secret 값을 기반으로 만든다

- Session Key는 대칭키가 된다

1-4. **Server 인증 확인**

- 서버는 개인키로 복호화하여 Premaster Secret값 획득
- Master Secret 및 Session Key 생성
- Session Key는 대칭키가 된다.

1-5. **Handshaking 종료**

2. **데이터 전송**

서버와 클라이언트는 Session Key를 활용해 데이터를 암호화 / 복호화 하여 데이터를 송수신

3. **연결 종료 및 Session Key 폐기**
