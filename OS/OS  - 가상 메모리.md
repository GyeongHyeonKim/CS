### 배경지식

#### Demand Paging

---

![](https://velog.velcdn.com/images/sujipark2009/post/5369e2ea-b00d-4eb2-8629-1338352f4ce5/image.png)

물리적인 메모리의 주소변환은 OS가 관여하지 않는다고 했었다.

가상메모리 기법은, 전적으로 **OS가 관여하는 기법**이다.

여기서부터는 페이징 기법을 사용한다고 가정한다.

**Demand Paging**은, 페이지가 요청이 되었을 때 페이지를 메모리에 올려놓는 기법이다.

이것으로 인해 I/O의 양을 줄일 수 있다.
좋은 프로그램일수록 예외처리를 위한 코드가 많이 있는데, 그런 코드들은 가끔씩만 사용이 된다.

그런 코드들도 메모리에 올려놓게 된다면, 메모리를 많이 잡아먹게 된다.

만약, Demand Paging 기법을 사용하면 필요한것만 메모리에 올리기 때문에 I/O의 양이 감소하게되고, 물리적 메모리를 사용하는 양이 감소하게 된다.

페이징 기법에서 페이지 테이블 엔트리마다 valid/invalid bit이 있다고 했었다.

![](https://velog.velcdn.com/images/sujipark2009/post/c56a601c-e9f0-42cf-89bf-56ada67ea984/image.png)

지금 당장 필요한 페이지는 demand paging에 의해 메모리에 올라가있고, 그렇지 않은것은 backing store에 저장되어있다.

store에 저장되어있는 나머지 페이지에 대해서도 bit가 invalid로 표시되어있다.
또한, 사용이 안되는 페이지에 대해서도 invalid로 표시되어 있다.

실제 사용되는 페이지와 그렇지 않은 페이지가 있는데, 더 넉넉한 주소공간을 지원해주기 때문에 실제 사용이 안되는 G,H에 대해서도 invalid로 표시되어있다.

CPU가 논리주소를 주고 메모리 몇 번지를 읽겠다! 하고 왔는데 invalid라면?
이 말은, 이 페이지가 메모리에 없다는 말이다.
그 페이지를 먼저 Disk에서 올려야하고 이것은 I/O 작업이므로 사용자 프로세스가 할 수 없는 작업이다.

이것은 **OS가 해야한다**

이렇게 Page가 없는 상황에 `Page fault`라는 Trap이 걸리게 되고, 이때 OS가 CPU를 받아 해당 page를 메모리에 올리게 된다.

#### Page Fault

---

![](https://velog.velcdn.com/images/sujipark2009/post/e7164e3e-3b71-4963-b1d6-176403852049/image.png)

invalid page를 접근하면, MMU(주소변환을 위한 HW)가 Trap을 발생시킨다.
그러면 CPU가 OS에게 넘어가게되고 OS의 Page fault를 처리하는 코드가 다음의 순서로 실행이 된다.

1. 유효한 주소가 맞는지(이 프로세스가 접근할 수 있는 주소가 맞는지)

2. 빈 페이지 하나를 획득해야하는데, 없으면 하나를 쫓아내야 한다.

3. Disk에서 메모리로 그 페이지를 올리게 된다.
   3-1. 이 작업은 느린 작업으로, Disk I/O를 하게 되면 Block상태가 되어 CPU를 뺏기게 된다.
   3-2. Page table Entry에다가 valid로 표시한다.
   3-3. Ready Queue에 해당 Process를 넣는다.
4. 이 프로세스가 CPU를 받으면 Running 상태가 된다.

![](https://velog.velcdn.com/images/sujipark2009/post/83516226-7129-4647-97ee-4620e57e47fd/image.png)

그림을 보면, 주소 변환을 하려고 페이지 테이블을 봤더니 `invalid`라고 표시되어있는 상황이다.

페이지가 현재 메모리에 올라와있지 않다는 의미이니, trap이 걸려서 OS에게 CPU가 넘어가게 된다.

OS는 Backing store에 있는 page를 물리적 메모리로 올려놓는데 이때, 빈 페이지 프레임이 없으면 하나를 쫓아내게 된다.

그리고 페이지를 메모리에 올리고 Page를 valid로 변경한다.

그리고 다시 CPU를 얻어서 주소변환을 하면 valid로 되어있으므로, 실제 물리적 메모리에 접근할 수 있게 된다.

![](https://velog.velcdn.com/images/sujipark2009/post/971ce236-8ba4-4c48-9367-df5bdef42759/image.png)

Page fault시 Disk에 접근하는것은 매우 느리다.

Fault가 발생하면 OS에게 CPU가 넘어가서 Fault를 처리해야하고, 그 과정에서 빈 페이지가 없으면 쫓아내야하고, valid로 표시해야하고, CPU를 얻으면 running하는 이런 overhead가 존재한다.

![](https://velog.velcdn.com/images/sujipark2009/post/85302128-8f0b-4875-93a9-13be2bed15c5/image.png)

빈 프레임이 없으면 쫓아내야한다고 했는데, 그것을 `Page Replacement`라고 한다.

이것은 OS가 하는 일로, 어떤 페이지를 메모리에 쫓아내고 새 페이지를 올릴지를 결정한다.
이것을 위한 알고리즘을 `Replacement Algorithm`이라고 한다.

페이지를 쫓아냈더니 다시 그 페이지를 필요로 한다면, 엄청난 손해를 입기 때문에 이 알고리즘은 **가급적 Page fault rate가 낮아지도록** 동작해야한다.

![](https://velog.velcdn.com/images/sujipark2009/post/4195b731-8847-4c4f-83e1-b8ce1a4a13a6/image.png)

그림처럼 내려갈 페이지(victim)을 결정했으면 디스크로 쫓아내야한다.

그런데, 이 페이지가 디스크에서 메모리로 올라온 이후로 내용이 변경되었다면 그냥 쫓아내는게 아니라 변경된 내용을 Backing store에 써줘야한다.

변경된게 없다면 그냥 지우기만 하면 된다.

![](https://velog.velcdn.com/images/sujipark2009/post/196f2527-1f6a-4a86-a642-da8fd6e2f774/image.png)

어떤 알고리즘이 가장 좋은 알고리즘일까?

**Optimal Algorithm**은 미래에 어떤 페이지가 올 지 알고있다는 가정하에 사용할 수 있는 알고리즘이다.

현실성이 없는 알고리즘으로, 가장 미래에 참조될 페이지를 쫓아내는 방식이다.

이 경우, 6번의 Page fault가 발생하는데, 이게 최적이기에 이것보다 Fault수를 줄일 수는 없다.
다른 알고리즘의 성능에 대한 `Upper Bound`를 제공한다.

아무리 좋은 알고리즘을 만들어도, 이것보다 좋을수는 없다는 것이다.

여기서부터의 알고리즘은, 실제 시스템에서 사용가능하고 미래를 모른다는 가정하에 사용할 수 있는 알고리즘이다.

![](https://velog.velcdn.com/images/sujipark2009/post/9411ff46-0bad-4cac-aab5-98371474fb37/image.png)

FIFO(First in First out)

말 그대로 먼저 들어온 것을 내쫓는 방식이다.

이 알고리즘은 특이한 성질이 있는데, 메모리 프레임의 수를 늘려주면 성능이 더 좋아져야 하는데 오히려 Page fault가 늘어나는 현상이 있다.

#### LRU(Least Recently Used)

---

![](https://velog.velcdn.com/images/sujipark2009/post/ccf0c943-a2a0-470b-b18f-96eb49c384d6/image.png)

가장 널리 사용되는 알고리즘이다.
**가장 오래된 페이지를 쫓아내는 알고리즘**이다.

#### LFU(Least Frequently Used)

---

![](https://velog.velcdn.com/images/sujipark2009/post/07849783-65df-47e7-966f-e1c49e94d3f3/image.png)

**가장 덜 빈번하게 사용된 페이지를 쫓아내는 방식**이다.

과거에 참조횟수가 많았던 페이지는 미래에도 많을 수 있으므로 쫓아내지 말자는 것이다.

동률인 경우, 임의로 선정할 수 있고 성능향상을 생각한다면 가장 참조시점이 오래된 page를 지우도록 구현할 수 있다.

![](https://velog.velcdn.com/images/sujipark2009/post/0fc8005d-c193-4ca9-9950-7a805c707b16/image.png)

Reference String이 1,1,1,1,2,2,3,3,2,4,5라고 해보자.

그리고 4개의 페이지 프레임이 있다고 해보자.

저 현재시각시점에 5번페이지를 참조해야하는 상황이라고 해보자.

LRU의 경우, 1번 페이지를 쫓아내게 된다.

- 마지막 참조 시점만 본다는 약점이 있다.
- 과거에 많이 참조했었다는 것을 반영하지 못한다.

LFU의 경우, 4번 페이지를 쫓아내게 된다.

- 비록 4번의 횟수가 1번이지만, 이제 막 참조하려는 상황일수도 있다.

![](https://velog.velcdn.com/images/sujipark2009/post/52478708-cd23-4764-8b10-6f2832ade44e/image.png)

LRU 알고리즘은 메모리 안에있는 페이지들을 시간순서에따라 줄 세우기를 한다.
일종의 **LinkedList**의 형태이다.
메모리 내에서 재참조 되거나, 새로운 페이지가 들어오면 가장 아래로 보내면 된다.

쫓아낼때는 가장 위에있는 것을 쫓아내면 된다.
이렇게 하면 O(1)의 복잡도로 구현이 가능하다.

LFU 알고리즘은 비슷하게 한 줄로 줄세우기를 하는데
가장 참조횟수가 적은것이 위에오고 많은것이 아래에 위치한다.

사실 LFU는 한 줄로 줄세우기를 할 수 없는데, 지금 참조되었다는건 참조횟수가 1이 늘어났다는 의미이지 그렇다고 해서 가장 밑으로 내려갈 수 있는게 아니라, 비교를 해서 아래로 내려가면서 위치를 찾아야하기 때문이다.

최악의 경우 O(N)이 될 수 있다.

![](https://velog.velcdn.com/images/sujipark2009/post/0714d9c0-4d27-454c-8e1b-650cdabcc93b/image.png)

그래서 사실 Heap 자료구조를 사용해서 구현하게 된다.

맨 위는 참조횟수가 가장 적은 페이지, 밑으로 갈수록 자식이 부모보다 참조횟수가 더 많은 페이지들이 있다. 일종의 최소힙이라고 할 수 있겠다.

저 높이가 logN이 되기에, 비교횟수가 많아봤자 logN이 된다.

그런데, 실제 페이징 시스템에서는 LRU,LFU는 사용할 수 없다.

왜 그럴까?

![](https://velog.velcdn.com/images/sujipark2009/post/f7780375-d7f1-4b40-8998-e427370d5eac/image.png)

Process A가 지금 CPU를 가지면서 running을 하고 있다고 해보자.

그림은 A라는 프로세스의 논리적 메모리, 페이지 테이블, 물리적 메모리, 그리고 backing store이다.

이 프로그램이 메모리 참조를 하기위해서 주소변환을 했더니, 그 내용이 이미 물리적 메모리에 올라와있다면 그 페이지를 바로 CPU가 가지고 갈 것이다.

이렇게 CPU가 메모리에 올라와있는 것을 그냥 가져가게되면, 중간에 OS가 개입하는게 전혀 없어진다.

다시말해서, **이 페이지를 언제 사용했는지, 그 사실을 OS가 알 수 없다**는 것이다.

LRU 알고리즘을 운영하려면, 그 페이지들이 사용되는 시점을 알아야하는데, 저렇게 바로 가져가게 된다면 OS는 알 방법이 없다.

반대로 A라는 프로그램이 주소변환을 해본 결과 페이지가 없다면, 그런 경우에는 Page fault가 발생하고 CPU제어권이 OS에게 넘어갈 것이다.

OS가 Backing Store에 있는 페이지를 올려놓게 되고 그때 언제 올린지 시간을 알 수 있게 되므로, 그 페이지를 LRU 하단에 매달아 놓을 수 있게된다.

그래서 페이징 시스템에서 OS는 **페이지 정보를 반만 알고있다**고 할 수 있다.
처음 사용이 될 시점에만 기록할 수 있기 때문이다.

그래서 **실제로 페이징 시스템에서는 LRU,LFU 알고리즘을 사용할 수 없기 때문에, LRU를 근사시킨 알고리즘인 `Clock Algorithm`**을 사용한다.

![](https://velog.velcdn.com/images/sujipark2009/post/ca9c4ab6-9753-40c5-b74d-5e80aecb694b/image.png)

사각형 하나하나는 메모리 안의 페이지를 의미한다.

Replacement algorithm이므로, 이 페이지들 중 하나를 쫓아내야하는 상황이다.

각 페이지마다 0또는 1이 적혀있는데, 이 bit를 `referece bit` 혹은 `access bit`이라고 한다.

1이라고 한다면, 최근에 사용된 것이고 0이면 최근에 사용이 안된 페이지라는 의미이다.
LRU의 경우, 가장 오래된 페이지를 쫓아낸다고 했었다.

하지만 모든 페이지들의 사용시점을 OS가 다 알수는 없기에 bit하나만 가지고 쫓아낼 페이지를 결정하게 된다.

그래서 0으로 표시된 페이지를 쫓아내게 된다.

0으로 표현되었다고 해서 가장 오래된 페이지는 아니지만, 적어도 최근에 사용이 안된 페이지는 맞다.

이걸 구현하는 방법을 알아보자.

Reference bit을 1로 만드는것은 HW가 하는 일이다.
HW가 CPU가 주소변환을 해서 그 페이지를 가져갔다! 하면 1로 두는 것이다.

근데 OS가 이 중 하나를 쫓아내야겠다고 한다면 시계바늘이 돌면서 bit가 1이면 0으로 바꾸고 다음 칸으로 화살을 이동하게 된다.

그러다가 만약0이라면 쫓아내게 되는 것이다.

bit를 1로 바꾸는건 HW가, 0으로 바꾸는건 OS가 하게 된다.

이렇게 하면 bit가 1이라는 의미는 시계바늘이 한바퀴 돌아오는 동안 적어도 그 페이지가 한번은 사용되었다는 의미이다.

0은 한 번 돌아올때까지 사용이 안되었다는 의미이므로 쫓아내게 된다.

reference bit 말고 `modified bit / dirty bit`도 있다.

이 메모리 페이지가 사용되는건 CPU의 read 혹은 write가 있는데

페이지가 올라온 다음에 CPU가 접근을 하면 HW가 reference bit을 1로 바꾸어 주고 CPU가 write를 해서 수정하게 된다면, 그 페이지에 대한 modified bit을 1로 바꾸게 된다.

이 modified bit을 왜 쓰냐면, 저 페이지가 올라온 이후로 수정이 되었으면 쫓아낼 때 그냥 없애는게 아니라 disk에 써줘야하기 때문에 그것을 알기위한 목적이다.

#### Page Frame Allocation

---

![](https://velog.velcdn.com/images/sujipark2009/post/f3c7831f-4235-4b7f-bb3f-8efd17bc384d/image.png)

앞서 설명한 알고리즘은 어떤 프로그램에 의해서 사용되는 페이지인지는 전혀 고려하지 않았다.

물리적 메모리에는 A라는 프로세스의 페이지 / B라는 프로세스의 페이지.. 이렇게 다 올라가있다.

쫓아낼 때는 LRU 알고리즘을 사용한다면 그 중 가장 오래된 페이지를 쫓아내게 되는데
프로세스 A가 사용한다고 해서 쫓아내고, B가 쓰는거라 봐주는 등 그런게 없었다.

여기서 말하는 Allocation 이라는 의미는, 프로세스 A에게 이만큼, B에게 이만큼을 할당하겠다는 의밍이다.

할당 이야기를 하는 이유는, 프로그램이 원활하게 실행되려면 적어도 메모리 상 페이지 프레임을 몇 개는 가지고 있어야하기 때문이다.

만약 지금 반복문을 돌고있다고 해보자. 그 반복문을 구성하는 페이지가 5개이고 5개의 페이지를 할당했다면 반복문을 1천만번을 돌아도 Page fault가 안 날 것이다.

그런데 만약 3개만 줬다면, 반복문을 도는 동안 계속 Page fault가 날 것이다.

그것은 대단히 비효율적이기 때문에, **각 프로세스 당 최소로 가져야하는 프레임**이 있다.

그래서 할당이 필요하다는 것이다.

프로그램을 구성하는 **code,data,stack**이 있는데, code만 올라가있다고 해서 fault가 안나는것이 아니고, data도 올라가 있어야 한다.

할당 방식은, `Equal Allocation`과 ,`Preportional Allocation`,`Priority Allocation`이 있는데

Equal의 경우, 모든 프로세스에 공평한 개수를 할당하는 것이고

Preportional은 프로세스 크기에 비례해서 할당,

Priority의 경우 CPU 우선순위가 높은 프로세스에게 더 많은 페이지 프레임을 할당하는 것이다.

#### Thrashing

---

![](https://velog.velcdn.com/images/sujipark2009/post/fc41f8f0-acbe-4a9c-810c-be519dfe12ac/image.png)

x축은 메모리에 올라가있는 프로그램의 수
y축은 CPU사용률(CPU가 단위시간당 일한 비율) 이다.

메모리에 프로그램 하나가 올라가있으면 그 프로그램이 CPU를 쓸 것이다.
그렇게 쓰다가 I/O를 하러 가버리면 CPU가 놀기때문에, CPU 사용률이 낮아진다.

이런식으로 프로세스의 수를 증가시키면 사용률이 조금씩 증가하다가, 어느 순간 떨어지는데 이걸 `Thrashing` 이라고 한다.

메모리에 너무 많은 프로그램을 동시에 올려놓읜, 프로그램이 원활하게 운용되기 위해 필요한 최소한의 메모리도 얻지 못한 상황인 것이다.

그래서 CPU를 줘도 계속 Page fault가 나는 상황이다.
CPU는 놀고있고, Page fault를 처리하느라 바쁜 상황이다.

![](https://velog.velcdn.com/images/sujipark2009/post/a44efe1e-8c27-4a60-82ad-5942e2bc661a/image.png)

Thrashing이 발생해서 CPU가 놀고있으면, OS는 프로그램을 더 넣어야겠다고 판단하고 결국 시스템이 더 안좋아지게 된다.

CPU는 일을 못하고, I/O만 하게되는 상황이 발생한다.

그래서 thrashing을 막아야하는데, 이것을 막는 방법은 그 프로그램이 필요로 하는 최소한의 메모리는 보장해주는 것이다.

![](https://velog.velcdn.com/images/sujipark2009/post/bd5772f9-45eb-4a43-ba61-9beff7a0fe08/image.png)

프로세스는 특정 시간동안에 특정 메모리를 집중적으로 사용하는 경향이 있다.

예를들어 어떤 함수를 실행하고있다하면, 그 함수가 있는 페이지가 집중적으로 사용이 될 것이다.

그런 집중적으로 사용되는 page들의 집합을 Locality set이라 하고 이걸 보장해 줘야 원활하게 프로그램이 동작한다는 것

그런데, Working-set Model에서는 그러한 locality set의 집합을 working set이라고 부른다. 사실 둘이 같은말임..

working set을 이용한 메모리 관리 알고리즘은, 저 working set에 포함된 페이지들을 무조건 메모리에 보장하도록 작동한다.

만약 5개가 필요한데 빈 공간이 3개밖에없다?

그럼 그냥 메모리를 다 반납하고 swap out 되어라(suspended)하는 식으로 작동한다.
나중에 메모리가 좀 남아돌때 다시 제공받게 된다.

![](https://velog.velcdn.com/images/sujipark2009/post/3248f5a3-7575-438d-a758-9950fb75e993/image.png)

지금 보면 시간순서에따라 page reference string이 있는데

이건 특정 프로세스의 페이지이다.

그래서 working set이 뭔지는 정확하게 모르지만, 과거를 통해 추정을 한다.

현재 시점을 기준으로 working set을 결정하게 된다.

델타만큼의 시간동안 사용된 페이지를 유지하게된다.

대충.. 너무 깊게는 .. 알기싫네...

Working set은 Global 과 Local replacement를 적절히 섞은..것이라고 볼 수 있다.

![](https://velog.velcdn.com/images/sujipark2009/post/de3b7e75-333a-4da4-abf8-8b0e2ed33526/image.png)

보통 32bit 시스템에서는 4KB의 페이지 크기를 사용하는데

이제 64bit 주소체계를 쓰면 어떻게 해야할까?

메모리가 커지면서 메모리를 쪼개쓰는 페이지크기도 더 커지는 추세로 가게 된다.

그러면 페이지 크기를 키우거나 줄이거나 했을 때 전체 시스템에 어떤 영향을 미칠까?

페이지 크기를 줄이면?

기존에

![](https://velog.velcdn.com/images/sujipark2009/post/6c847373-8853-4e90-adbe-ca48dd7f7dff/image.png)

8개로 쪼개던걸 100개로 쪼개면?..

페이지 테이블이 그만큼 엔트리가 늘어날것이다.

대신에 물리적 메모리도 작게 썰어버리니까 불필요한 공간은 사라질것임(내부 단편화가 줄어들듯)

대신에 잘게 쪼게면 단점이 페이지 테이블이 더 늘어나야 한다는 문제가 있고

보통 페이지가 사용이 되면, 어떤 위치가 사용되면 그 인접한 위치가 사용될 확률이 높은데,.. 그런 관점에서 페이지 폴트가 많이 날 것이다.

그말이 Disk transfer의 효율성이 감소한다는 것이다.

기존에 크게 읽었으면 한번에 읽어올걸 작게 쪼개놔서 여러개를 읽어야하니까..
Locality 측면에서 좀 손해다.

### 메모리 관리 기법에는 무엇이 있나요?

---

**`연속 메모리 할당`**

- 프로세스 크기에 맞춰 메모리에 연속적으로 할당하는 기법

구현이 간단 / 메모리 관리 overhead가 적다

그러나, 메모리 할당과 제거를 반복하면서 외부 단편화가 발생한다는 문제점이 있다.

**종류**

**고정 분할 기법**

- 메모리 영역을 여러 개의 고정된 크기로 분할하여 프로세스에 제공

내부 단편화 발생 및 프로세스가 파티션보다 크면 배치가 불가능하다

**동적 분할 기법**

- 각 프로세스의 크기에 따라 메모리를 분할

- 외부 단편화 발생

시간이 지나면서 메모리에 빈 공간이 산발적으로 분포하게 됨

compaction이라는 해결책이 있으나, 비용이 많이든다.

#### 메모리의 연속할당 방식 세 가지를 설명해주세요

---

**First-fit**

- 가장 처음 만나는 빈 메모리 공간에 프로세스를 할당
  속도가 가장 빠르나 빈 공간이 앞에 몰리는 외부 단편화 발생

**Best-fit**

- 요청된 크기와 가장 근접한 크기의 메모리를 선택

빈 공간을 효율적으로 사용할 수 있으나, 탐색 시간이 길고 작은 빈 블록이 많이 남아 활용성이 떨어진다

**worst-fit**

- 가장 큰 빈 공간에 할당

다른 프로세스가 들어갈 공간을 남길 수 있으나, 외부단편화의 문제발생

#### worst-fit은 언제 사용할 수 있을까요?

---

- 메모리에 여유공간이 많을 때
- 연속적인 큰 프로세스를 실행할 가능성이 적을 때
- 메모리 할당 / 해제가 잦지 않은 경우

#### 성능이 가장 좋은 알고리즘은 무엇일까요?

---

일반적으로 First-fit이다.
Best-fit은 공간을 효율적으로 쓰지만 느리다.

**`불연속 메모리 할당`**

- 메모리 공간을 연속된 영역이 아닌 여러 비연속적인 영역에 할당하는 방식

외부 단편화 문제를 해결할 수 있고 가상 메모리와의 호환성이 좋다는 장점

**단점**

- 메모리 관리 overhead가 큼
  논리 주소를 물리 주소로 변환하는 오버헤드

- 내부 단편화
  프로세스 마지막 페이지에 내부 단편화

**종류**

- 페이징

고정 크기의 페이지로 메모리에 할당

- 세그멘테이션

서로 크기가 다른 세그먼트로 나눠서 메모리에 할당
의미단위로 나누는 것

#### 메모리 관리 기법이 필요한 이유는 무엇인가요?

---

- 메모리 효율

프로그램의 크기가 지속적으로 증가 -> 메모리의 효율적 사용이 필요함

- 다중 프로그래밍

여러 프로그램을 동시에 실행하기 위해

- 보호 및 격리

각 프로세스의 주소 공간을 보호하고 격리하는 방법을 제공
한 프로세스가 다른 프로세스를 방해하여 오류나 충돌이 발생하는 것을 방지

### Demand Paging이란 무엇인가요?

---

- 현재 필요한 페이지만 메모리에 올리는 것

- CPU가 요청할 때 프로세스의 데이터를 메모리에 올리는 것

**특징**

- 처음부터 모든 데이터를 메모리로 적재하지는 않음

**반대**

- 선행 페이징

프로세스 관련 모든 데이터를 물리 메모리에 미리 적재 후 실행 - 하드웨어 제한으로 불가능

#### Page Fault란 무엇인가요?

---

- 페이지에 접근하려고 했을 때 해당 페이지가 실제 물리 메모리에 부재하는 경우 발생하는 인터럽트

- 어떤 프로그램이 가상 메모리에는 존재하지만 시스템의 RAM에는 현재 존재하지 않는 데이터 / 코드에 접근을 시도할 경우 발생하는 현상

![](https://velog.velcdn.com/images/sujipark2009/post/59a84a68-bc11-410c-9335-1f294448196d/image.png)

- 프로세스 A는 page2에 있는 정보가 필요하여 페이지2에 존재하는 가상 주소에 접근
- 가상 주소에서 물리 주소를 얻기 위해 페이지 테이블에 접근

- valid bit이 0인경우, 물리주소가 없고 이는 물리 메모리에 필요한 데이터가 적재되어 있지 않다는 뜻이다.

- 물리 메모리에 데이터를 올리고 , 페이지 테이블을 업데이트
- 데이터를 읽어서 해당 프로세스를 처리

#### Page Fault가 발생했을 때, 어떻게 처리하는지 설명해주세요

---

- 페이지 폴트가 발생하면 페이지를 메모리로 가져온 뒤 다시 동일한 명령을 수행

![](https://velog.velcdn.com/images/sujipark2009/post/5af0f13b-53cf-45a6-993c-67ae073c28ef/image.png)

1. CPU는 명령어를 실행하는 과정에서 메모리에 접근, 이때 가상 주소를 사용한다.

가상 주소를 물리 주소로 변환하기 위해 MMU에 요청

2. MMU는 먼저 TLB로 가서 그 가상주소에 대한 물리주소가 캐싱되어 있는지 확인

3. TLB에 캐싱된 물리주소

있으면 TLB Hits라 하고 MMU는 바로 물리 주소로 변환하여 메모리에서 데이터를 가져 옴

없으면 TLB Miss라 하고 MMU는 페이지 테이블을 직접 참조해야 한다.

4. MMU가 페이지 테이블에서 해당 가상 주소가 물리 메모리에 적재되어 있는지 valid bit을 확인한다.

5. valid bit의 값

1이면 MMU가 해당 페이지의 물리 주소를 이용하여 데이터를 가져옴

0이면 페이지 폴트가 발생하고 MMU가 페이지 폴트 인터럽트를 OS에 발생

6. OS는 해당 페이지를 저장공간에서 가져옴

7. OS는 저장공간에서 가져온 데이터를 메모리에 로드

물리 주소를 업데이트하고 페이지 테이블을 업데이트해준다(valid bit -> 1)

8. OS는 CPU에게 프로세스를 다시 실행할 것을 지시한다.

9. CPU는 다시 MMU에 가상 주소를 요청

- 이번에는 페이지 테이블에 업데이트 된 정보 존재
- MMU는 물리 주소로 변환하여 메모리로부터 데이터를 가져옴
- TLB도 업데이트 됨

#### 페이지 크기에 대한 Trade-Off를 설명해주세요.

---

**내부 단편화**

- 페이지에서 사용되지 않는 공간

프로세스가 2KB의 메모리를 요청했지만, 페이지 크기가 4KB라면 2KB의 메모리가 낭비

페이지 크기를 크게 설정하면, 내부 단편화(Internal Fragmentation)가 늘어남

**Page Table Size**

- 페이지 크기를 작게 설정하면 페이지 테이블의 크기가 커질 수 있음

페이지가 많아지기 때문이다.

- 가상 주소를 물리 주소로 매핑하는 데 필요한 정보의 양이 증가

- 메모리 사용량을 증가시키고, 페이지 테이블을 관리하는 데 드는 오버헤드가 증가

**Disk I/O **

- 페이지 크기가 크면 디스크 I/O 작업이 줄어들 수 있음

디스크에서 페이지를 가져오거나 디스크에 페이지를 쓸 때, 한 번에 더 많은 정보를 이동할 수 있음

페이지 폴트가 발생했을 때, 필요 이상의 데이터를 메모리에 로드하게 될 수 있음

**TLB Hits**

- 더 작은 페이지 크기는 Translation Lookaside Buffer(TLB)에 더 많은 엔트리를 요구

- TLB Hit rate가 감소할 수 있음

#### 페이지 크기가 커지면, 페이지 폴트가 더 많이 발생한다고 볼 수 있나요?

---

알 수 없다

**페이지 폴트 발생 빈도 요소**

- 페이지 크기
- 메모리 접근 패턴 => 원하는 데이터가 많은 페이지에 퍼져있을 가능성
- 캐시 효율성
- 내부 단편화

**큰 페이지 크기**

**장점**

- 페이지 테이블 항목 수 줄임
- 지역성을 높임
- TLB Hits 증가

**단점**

- 내부 단편화 증가
- 메모리 낭비 유발
- 페이지 폴트 처리 시간 증가(크기가 크기에)

#### MMU란 무엇인가요?

- 가상주소를 물리주소로 변환하고, 메모리를 보호하는 기능을 수행

**역할**

- 동적 주소 변환
  논리주소를 물리주소로 변환

- 페이징
- 고정/동적 분할

**원리**

- CPU가 각 메모리에 접근하기 이전에 메모리 주소 번역 작업이 수행
- 메모리를 일일이 가상 주소에서 물리적 주소로 번역하게 되면 작업 부하가 너무 높아짐

MMU는 RAM을 여러 부분(페이지,pages)으로 나누어 각 페이지를 하나의 독립된 항목으로 처리

- 페이지 및 주소 번역 정보를 기억하는 작업이 가상 메모리를 구현하는데 있어 결정적인 절차

#### TLB란 무엇인가요?

---

- 가상 메모리 주소를 물리적 주소로 변환하는 속도를 높이기 위해 사용하는 캐시
- Translation Lookaside Buffer

- 최근에 사용된 페이지 테이블 항목의 복사본을 저장
- 동일한 가상 주소가 다시 참조될 때 페이지 테이블을 메인 메모리에서 찾아야 하는 대신, 더 빠른 TLB에서 찾을 수 있음

- CPU가 가상 주소를 가지고 메모리에 접근하려고 할 때, 우선은 TLB에 접근하여 가상주소에 해당되는 물리주소를 찾음

- 만약 TLB에 매핑이 존재하지 않는다면, MMU가 페이지 테이블에서 해당되는 물리 주소로 변환한 후 메모리에 접근

#### TLB를 쓰면 왜 빨라지나요?

---

- 물리주소를 갖고 있으면 메모리에 두 번 들릴 필요없이, 바로 해당 물리주소를 찾아갈 수 있음

- 가상주소를 물리주소로 변환하는 과정에서 발생하는 오버헤드를 줄임

가상 메모리 시스템에서 주소 변환은 페이지 테이블을 통해 수행됨

페이지 테이블은 일반적으로 메인 메모리에 위치

가상 주소를 물리 주소로 변환하기 위해서는 일반적으로 메인 메모리에 접근해야 한다

메인 메모리에 접근하는 것은 상대적으로 느린 연산

동일한 가상 주소가 다시 참조될 때 페이지 테이블을 메인 메모리에서 찾아야 하는 대신 더 빠른 TLB에서 찾을 수 있다.

#### TLB와 MMU는 어디에 위치해 있나요?

---

TLB

- MMU 내부에 위치한다
- 메모리에 접근하는 대신 TLB에 접근하여 자기 안에서 처리하도록 한다.

캐시이기 때문

MMU

- CPU 코어 내부에 속하거나, 가까운 곳에 별도의 칩으로 존재
- 메모리 주소 변환 과정이 프로세스 실행 속도에 큰 영향을 미치기 때문

#### 세그멘테이션과 페이징의 차이점은 무엇인가요?

---

**페이징**

- 메모리 공간이 연속적으로 할당되어야 한다는 제약조건을 없애는 메모리 관리 전략

- 물리 메모리(메인 메모리)를 동일한 크기의 블록인 페이지로 나눔

- 페이지 테이블을 통해 매핑되어 가상주소를 물리주소로 변환하는데 사용

**원리**

- 페이지 - 논리메모리 - 프로세스가 사용하는 공간을 논리 메모리에서 여러 개의 페이지로 나누어 관리

- 프레임 - 물리메모리 - 개별 페이지는 순서에 상관없이 물리 메모리에 있는 프레임에 매핑

- 페이지 테이블

논리적 페이지와 물리적 프레임 간의 매핑은 페이지 테이블에서 유지

- MMU의 재배치 레지스터 방식을 활용해 CPU가 마치 프로세스가 연속된 메모리에 할당된 것처럼 인식

**단점**

내부 단편화 발생

**세그멘테이션**

- 페이징 기법과는 반대로 논리 메모리와 물리 메모리를 같은 크기의 블록이 아닌, 서로 다른 크기의 논리적 단위인 세그먼트로 분할

- 논리적 단위의 예시에는 함수,데이터 구조, 배열이 있다.

**목표**

- 메모리를 논리적 단위로 분할

프로그램이 메모리를 사용하는 방식을 더 잘 반영할 수 있기 때문이다.

**장점**

내부 단편화 최소화

**단점**

외부 단편화 발생 - 빈 메모리 공간이 세그먼트보다 작은 경우 할당할 수 없음

**세그멘테이션과 페이징의 혼용**

- 페이징과 세그멘테이션도 각각 내부단편화와 외부 단편화가 발생

- 페이징과 세그멘테이션을 혼용해 이러한 단편화를 최대한 줄이려는 전략

- 프로세스를 세그먼트(논리적 기능 단위)로 나눈 다음 세그먼트를 다시 페이지 단위로 나누어 관리

- 매핑 테이블을 두 번 거쳐야하므로 속도가 느려짐

#### 어떤 주소공간이 있을 때, 이 공간이 수정가능한지 알 수 있는 방법이 있나요?

---

- 주소 공간이 수정 가능한지 여부는 OS가 관리
- 해당 페이지의 속성이나 접근 제어 리스트 등을 통해 확인

- 페이징 시스템

각 페이지는 특정 플래그를 통해 읽기 / 쓰기 / 실행 등의 권한을 가질 수 있음

페이지 테이블에 저장

페이지 테이블을 확인함으로써, 해당 주소공간이 수정 가능한지 여부를 알 수 있음

#### 32bit에서 페이지의 크기가 1KB라면, 페이지 테이블의 최대 크기는 몇 일까요?

---

총 페이지의 수 : 2^32 / 2^10 = 2^22개

페이지 테이블 엔트리의 크기 : 4B

페이지 테이블의 크기 : 2^22 \* 2^2 = 2^24 = 16MB

#### 가상 메모리란 무엇인가요?

---

- 가상 메모리는 메모리가 실제 메모리보다 많아 보이게 하는 기술

- 프로세스가 실행될 때 메모리에 해당 프로세스 전체가 올라가지 않더라도 실행이 가능하다는 점에서 착안

- 필요한 부분만 메모리에 적재하는 것

- 빠르고 작은 기억장치(RAM)을 크고 느린 기억장치(디스크)와 병합

하나의 크고 빠른 기억장치(가상 메모리)처럼 동작하게 하는 것

#### 가상 메모리는 왜 필요하나요?

---

RAM의 크기보다 더 큰 프로그램을 실행하기 위해

#### 가상 메모리와 논리 메모리의 차이점은 무엇인가요?

---

- 논리 메모리

프로세스가 메모리를 보는 방식을 나타냄
프로세스는 물리 메모리의 실제 구조와 관계없이 일관된 주소 공간을 가짐
메모리를 할당하고 회수하는 데 필요한 추상화를 제공

- 가상 메모리

논리 메모리의 개념 확장
실제 물리 메모리의 크기를 초과하는 프로그램을 실행 가능하게 함

#### 가상 메모리가 가능한 이유가 무엇일까요?

- 물리적 메모리의 크기와 무관하게 프로그램이 더 큰 메모리 공간을 사용할 수 있도록 OS가 추상화를 제공하기에 가능

- 메모리 관리를 두 가지 다른 수준으로 분리

각 프로그램은 자신만의 전체 메모리 공간을 가진 것처럼 동작

- OS는 이 가상 메모리를 물리적 메모리에 매핑

이 과정은 MMU와 같은 하드웨어 구성요소에 의해 지원

- 프로그램이 필요로 하는 메모리만 물리 메모리에 로드

페이지나 세그먼트라는 단위

- 프로그램이 요청하는 메모리가 현재 물리 메모리에 로드되어 있지 않으면 페이지 폴트가 발생

OS는 해당 페이지나 세그먼트를 디스크로부터 물리 메모리로 로드

#### 가상 메모리를 사용할 때 장단점은 무엇인가요?

---

- 프로세스 독립성 강화

각 프로세스는 자신만의 가상 메모리 공간을 가지므로 다른 프로세스의 메모리 영역에 영향을 주지 않음

- 물리 메모리의 효율적 사용

실제로 사용되지 않는 메모리 영역을 디스크로 옮겨 물리 메모리의 사용량을 줄일 수 있음

- 커다란 주소 공간 제공

각 프로세스에게 물리 메모리보다 훨씬 큰 주소 공간을 제공
더 큰 메모리 공간을 사용할 수 있게 되어 복잡한 App의 개발이 용이

**단점**

- 가상 메모리 시스템은 매핑, 페이지 폴트 처리,메모리 관리 등 복잡한 작업을 수행

OS의 복잡성을 증가시키며, 잠재적인 오류 발생 가능성을 높임

- 성능 오버헤드

메모리 엑세스 시간이 늘어남 , 디스크 I/O가 증가

- 페이지 폴트

가상 메모리에서 필요한 페이지가 실제 물리 메모리에 없는 경우 페이지 폴트가 발생
페이지 폴트 처리는 비용이 많이 드는 작업
자주 발생하면 시스템 성능이 저하

#### 페이지 교체 알고리즘에 대해 설명해주세요.

---

- Page Fault가 발생했을때, 새로운 페이지를 메모리에 할당하기 위해 기존 페이지를 교체하는 방식을 결정하는 알고리즘

#### 페이지 교체 알고리즘은 왜 필요한가요?

---

- 페이지 폴트가 자주 일어날수록 OS의 성능이 많이 저하
- 페이지 폴트를 최소화 하기 위해 페이지 교체 알고리즘을 사용

#### 페이지 교체 알고리즘의 종류에는 어떤 것들이 있나요?

---

- FIFO

메모리에 가장 먼저 들어온 페이지를 먼저 교체, 가장 단순하지만 최적의 결과를 보장하지는 않는다

- OPT(Optimal Page Replacement)

미래의 페이지 요청을 미리 알고 있다는 가정하에, 가장 나중에 사용될 페이지를 교체하는 알고리즘

실제 시스템에서는 미래의 페이지 요청을 알 수 없기 때문에, 이론적인 모델로만 사용되며 벤치마킹 테스트에 사용된다.

- LRU(Least Recently Used)

가장 최근에 사용하지 않은 페이지를 교체

페이지의 참조 패턴을 추적하는데 필요한 하드웨어나 소프트웨어의 지원이 필요

- LFU(Least Frequently Used)

가장 적게 참조된 페이지를 교체하는 방법

페이지가 처음 로드되었을 때부터 그 페이지의 참조 횟수를 계속 추적하므로 구현이 복잡

- Clock

FIFO의 단점을 보완한 방법으로, 참조 비트를 사용해 최근에 참조된 페이지를 보호

페이지들은 원형 큐에 저장되고 "시계 바늘"이 해당 페이지가 최근에 참조되었는지를 체크하며 원형 큐를 따라 움직임

#### LRU 알고리즘은 어떤 특성을 이용한 알고리즘이라고 할 수 있을까요?

---

"지역성의 원리"

프로그램이 실행되는 동안 특정 부분의 코드나 데이터에 집중적으로 접근하는 경향

#### Thrashing 이란 무엇인가요?

---

- 컴퓨터의 메모리가 부족해서 페이지 교체가 빈번하게 일어나, 실제로 유용한 작업을 수행하는 시간보다 페이지 교체에 사용되는 시간이 더 많아지는 현상

- 프로세스의 크기가 사용가능한 메모리보다 훨씬 큰 경우
- 너무 많은 프로세스가 동시에 실행되며 메모리가 고갈되었을 때 발생

- 시스템은 대부분의 시간을 CPU가 실제 작업을 수행하는 것이 아니라, 페이지 교체를 위한 디스크 I/O에 사용

#### Thrashing 발생 시, 어떻게 완화할 수 있을까요?

---

- Thrashing 현상은 시스템의 메모리 부족으로 인해 발생하므로, 메모리 관리 전략을 개선함으로써 완화

- 메모리를 추가로 설치하거나 확장 -> 가장 직접적인 해결책

- 스왑공간 확장

스왑 공간은 디스크의 일부분을 메모리처럼 사용하는 것
스왑 공간이 충분하다면 메모리 부족 문제를 일시적으로 완화
디스크 I/O는 메모리 접근보다 훨씬 느리므로, 과도한 스왑 사용은 성능저하

- 멀티 프로그래밍 수준 조정

OS가 메모리에 로드할 수 있는 프로세스의 수를 조절
메모리에 너무 많은 프로세스가 로드되면, thrashing 현상이 발생할 가능성이 높아지기 때문

- 요구 페이징 기법 사용

프로세스의 모든 페이지를 한꺼번에 메모리에 로드하는 대신, 실제로 필요한 페이지만 메모리에 로드하는 demand paing 기법
